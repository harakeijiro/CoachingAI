"use client";

import { Suspense, useRef, useEffect, useState } from "react";
import { Canvas } from "@react-three/fiber";
import { Environment } from "@react-three/drei";
import { Model as Dog } from "@/components/chat/Dog";

type Message = {
  id: string;
  role: "user" | "assistant";
  content: string;
};

export default function ChatPage() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [isLoading, setIsLoading] = useState(false);

  // éŸ³å£°å…¥åŠ›é–¢é€£
  const recognitionRef = useRef<any>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [supportsSpeech, setSupportsSpeech] = useState(false);
  const [isContinuousListening, setIsContinuousListening] = useState(false);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const lastSpeechTimeRef = useRef<number>(0);
  const isManualInputRef = useRef<boolean>(false);

  // è¿½åŠ ï¼šç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆã®æ•´å½¢ãƒ»é‡è¤‡ã‚¬ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ¥ãƒ¼
  const normalize = (t: string) => t.replace(/\s+/g, " ").trim();
  const lastSentRef = useRef<string>("");
  const isProcessingRef = useRef(false); // é€ä¿¡ä¸­ãƒ•ãƒ©ã‚°
  const autoSendQRef = useRef<string[]>([]);
  const isSpeakingRef = useRef(false); // setIsSpeakingã«åŒæœŸ
  const COOLDOWN_MS = 250;

  const shouldSend = (t: string) => {
    const text = normalize(t);
    if (text.length < 5) return false; // ãƒã‚¤ã‚ºé˜²æ­¢
    if (text === lastSentRef.current) return false; // åŒä¸€æŠ‘åˆ¶
    lastSentRef.current = text;
    return true;
  };

  const tickAutoSend = async () => {
    if (isProcessingRef.current) return;
    const next = autoSendQRef.current.shift();
    if (!next) return;
    isProcessingRef.current = true;
    try {
      await handleAutoSubmit(next); // æ—¢å­˜ã®é€ä¿¡é–¢æ•°ã‚’å†åˆ©ç”¨
      await new Promise((r) => setTimeout(r, COOLDOWN_MS));
    } finally {
      isProcessingRef.current = false;
      if (autoSendQRef.current.length) tickAutoSend();
    }
  };

  const enqueueAutoSend = (text: string) => {
    autoSendQRef.current.push(normalize(text));
    tickAutoSend();
  };

  // éŸ³é‡ç›£è¦–é–‹å§‹
  const startVolumeMonitoring = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 256;
      microphone.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      microphoneRef.current = microphone;

      setIsMonitoringVolume(true);

      // éŸ³é‡ãƒã‚§ãƒƒã‚¯ã®é–“éš”ï¼ˆ50msï¼‰
      volumeCheckIntervalRef.current = setInterval(() => {
        if (isSpeaking && analyser) {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(dataArray);

          // RMSè¨ˆç®—
          let sum = 0;
          for (let i = 0; i < dataArray.length; i++) {
            sum += dataArray[i] * dataArray[i];
          }
          const rms = Math.sqrt(sum / dataArray.length) / 255;

          if (rms > volumeThreshold) {
            console.log("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—å§‹ã‚ã¾ã—ãŸã€TTSã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«:", rms);
            cancelSpeaking();
            // éŸ³å£°èªè­˜ã‚’å³åº§ã«å†é–‹
            if (isContinuousListening && !isManualInputRef.current) {
              setTimeout(() => {
                try {
                  recognitionRef.current?.start();
                  setIsRecording(true);
                } catch (e) {
                  console.log(
                    "Recognition restart after interruption failed:",
                    e
                  );
                }
              }, 100);
            }
          }
        }
      }, 50);
    } catch (error) {
      console.error("Volume monitoring failed:", error);
    }
  };

  // éŸ³é‡ç›£è¦–åœæ­¢
  const stopVolumeMonitoring = () => {
    if (volumeCheckIntervalRef.current) {
      clearInterval(volumeCheckIntervalRef.current);
      volumeCheckIntervalRef.current = null;
    }

    if (microphoneRef.current) {
      microphoneRef.current.disconnect();
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
    }

    setIsMonitoringVolume(false);
  };

  // éŸ³å£°åˆæˆï¼ˆTTSï¼‰é–¢é€£
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [supportsTTS, setSupportsTTS] = useState(false);
  const utteranceRef = useRef<SpeechSynthesisUtterance | null>(null);
  const voicesRef = useRef<SpeechSynthesisVoice[] | null>(null);
  const [audioEl, setAudioEl] = useState<HTMLAudioElement | null>(null);

  // éŸ³å£°ã‚­ãƒ¥ãƒ¼ï¼ˆè¤‡æ•°æ–‡ã®é †æ¬¡å†ç”Ÿç”¨ï¼‰
  const audioQueueRef = useRef<string[]>([]);
  const isPlayingQueueRef = useRef(false);

  // éŸ³é‡ç›£è¦–é–¢é€£
  const [isMonitoringVolume, setIsMonitoringVolume] = useState(false);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const volumeCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const volumeThreshold = 0.01; // éŸ³é‡ã®é–¾å€¤ï¼ˆèª¿æ•´å¯èƒ½ï¼‰

  // ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¯éŸ³å£°ã‚’å‡ºã—ã¦ã„ã‚‹æ™‚ã ã‘å£ã‚’å‹•ã‹ã™
  const isTalking = isSpeaking;

  // isSpeakingRefã‚’setIsSpeakingã¨åŒæœŸ
  useEffect(() => {
    isSpeakingRef.current = isSpeaking;
  }, [isSpeaking]);

  // Web Speech API åˆæœŸåŒ–
  useEffect(() => {
    const SR =
      (typeof window !== "undefined" &&
        ((window as any).SpeechRecognition ||
          (window as any).webkitSpeechRecognition)) ||
      null;

    if (SR) {
      setSupportsSpeech(true);
      const recognition = new SR();
      recognition.lang = "ja-JP";
      recognition.interimResults = true;
      recognition.continuous = true;

      recognition.onresult = (event: any) => {
        let interim = "";
        let finalText = "";
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) finalText += transcript;
          else interim += transcript;
        }

        // æ‰‹å‹•å…¥åŠ›ä¸­ã§ãªã„å ´åˆã®ã¿éŸ³å£°å…¥åŠ›ã‚’å‡¦ç†
        if (!isManualInputRef.current) {
          setInput((prev) => {
            const base = prev.replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, "");
            return finalText
              ? base + finalText
              : base + (interim ? `ï¼ˆè©±ã—ä¸­â€¦${interim}ï¼‰` : "");
          });

          // éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
          if (interim || finalText) {
            lastSpeechTimeRef.current = Date.now();
            if (silenceTimeoutRef.current) {
              clearTimeout(silenceTimeoutRef.current);
            }
          }

          // finalTextãŒç¢ºå®šã—ãŸå ´åˆã¯å³åº§ã«é€ä¿¡
          if (finalText && isContinuousListening) {
            const cleanText = finalText.trim();
            if (cleanText && shouldSend(cleanText)) {
              console.log("éŸ³å£°èªè­˜å®Œäº†ã€è‡ªå‹•é€ä¿¡:", cleanText);
              setInput("");
              enqueueAutoSend(cleanText);
            }
          } else if (interim) {
            // interimã®å ´åˆã¯ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®šï¼ˆ1.5ç§’ï¼‰
            silenceTimeoutRef.current = setTimeout(() => {
              if (isContinuousListening && !isManualInputRef.current) {
                setInput((currentInput) => {
                  const cleanInput = currentInput
                    .replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, "")
                    .trim();
                  if (cleanInput && shouldSend(cleanInput)) {
                    console.log("ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€è‡ªå‹•é€ä¿¡:", cleanInput);
                    setInput("");
                    enqueueAutoSend(cleanInput);
                  }
                  return currentInput;
                });
              }
            }, 1500);
          }
        }
      };

      recognition.onend = () => {
        setIsRecording(false);
        setInput((prev) => prev.replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, ""));

        // å¸¸æ™‚ãƒªãƒƒã‚¹ãƒ³ä¸­ãªã‚‰è‡ªå‹•å†é–‹
        if (isContinuousListening && !isManualInputRef.current) {
          setTimeout(() => {
            try {
              recognitionRef.current?.start();
              setIsRecording(true);
            } catch (e) {
              console.log("Recognition restart failed:", e);
            }
          }, 100);
        }
      };

      recognition.onerror = (event: any) => {
        console.log("Recognition error:", event.error);
        setIsRecording(false);

        // ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å¸¸æ™‚ãƒªãƒƒã‚¹ãƒ³ä¸­ãªã‚‰å†é–‹ã‚’è©¦è¡Œ
        if (
          isContinuousListening &&
          !isManualInputRef.current &&
          event.error !== "not-allowed"
        ) {
          setTimeout(() => {
            try {
              recognitionRef.current?.start();
              setIsRecording(true);
            } catch (e) {
              console.log("Recognition restart after error failed:", e);
            }
          }, 1000);
        }
      };

      recognitionRef.current = recognition;
    }

    return () => {
      try {
        recognitionRef.current?.stop();
      } catch {}
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
      }
      stopVolumeMonitoring(); // éŸ³é‡ç›£è¦–ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    };
  }, []);

  // å¸¸æ™‚ãƒªãƒƒã‚¹ãƒ³é–‹å§‹
  useEffect(() => {
    if (supportsSpeech && recognitionRef.current && !isContinuousListening) {
      setIsContinuousListening(true);
      setTimeout(() => {
        try {
          recognitionRef.current?.start();
          setIsRecording(true);
        } catch (e) {
          console.log("Initial recognition start failed:", e);
        }
      }, 500);
    }
  }, [supportsSpeech, isContinuousListening]);

  // TTS åˆæœŸåŒ–
  useEffect(() => {
    if (typeof window === "undefined") return;
    const hasTTS =
      "speechSynthesis" in window && "SpeechSynthesisUtterance" in window;
    setSupportsTTS(!!hasTTS);
    if (!hasTTS) return;

    const synth = window.speechSynthesis;
    const updateVoices = () => {
      const v = synth.getVoices();
      if (v && v.length) {
        voicesRef.current = v;
        // åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒœã‚¤ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆåˆå›ã®ã¿ï¼‰
        const jaVoices = v.filter(
          (voice) =>
            /ja/i.test(voice.lang || "") ||
            /æ—¥æœ¬èª|Japanese/i.test(voice.name || "")
        );
        if (jaVoices.length > 0) {
          console.log("ğŸ“¢ åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒœã‚¤ã‚¹:");
          jaVoices.forEach((voice) => {
            console.log(`  - ${voice.name} (${voice.lang})`);
          });
        }
      }
    };
    updateVoices();
    synth.onvoiceschanged = updateVoices;

    return () => {
      try {
        synth.onvoiceschanged = null as any;
      } catch {}
      try {
        synth.cancel();
      } catch {}
    };
  }, []);

  // TTS åˆ¶å¾¡
  const cancelSpeaking = () => {
    // ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢
    audioQueueRef.current = [];
    isPlayingQueueRef.current = false;

    try {
      audioEl?.pause();
    } catch {}
    try {
      // æ—¢å­˜ã® Web Speech å†ç”ŸãŒæ®‹ã£ã¦ã„ãŸã‚‰åœæ­¢
      if (typeof window !== "undefined") {
        window.speechSynthesis?.cancel?.();
      }
    } catch {}
    setIsSpeaking(false);
  };

  // éŸ³å£°ã‚­ãƒ¥ãƒ¼ã®æ¬¡ã®é …ç›®ã‚’å†ç”Ÿ
  const playNextInQueue = async () => {
    if (isPlayingQueueRef.current || audioQueueRef.current.length === 0) {
      return;
    }

    isPlayingQueueRef.current = true;
    const text = audioQueueRef.current.shift()!;

    try {
      // Cartesia TTS API ã‚’å‘¼ã³å‡ºã—
      const res = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text }),
      });
      if (!res.ok) throw new Error(`TTS ${res.status}`);

      const buf = await res.arrayBuffer();
      const blob = new Blob([buf], { type: "audio/wav" });
      const url = URL.createObjectURL(blob);
      const a = new Audio(url);
      setAudioEl(a);
      a.onplay = () => {
        setIsSpeaking(true);
        isSpeakingRef.current = true;
        try {
          recognitionRef.current?.stop();
        } catch {}
        setIsRecording(false);
        startVolumeMonitoring(); // éŸ³é‡ç›£è¦–é–‹å§‹
      };
      a.onended = () => {
        URL.revokeObjectURL(url);
        isPlayingQueueRef.current = false;
        onTtsEnded(); // çµ±ä¸€ã•ã‚ŒãŸçµ‚äº†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        // æ¬¡ã®éŸ³å£°ã‚’å†ç”Ÿ
        playNextInQueue();
      };
      a.onerror = () => {
        URL.revokeObjectURL(url);
        isPlayingQueueRef.current = false;
        onTtsEnded(); // çµ±ä¸€ã•ã‚ŒãŸçµ‚äº†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        // ã‚¨ãƒ©ãƒ¼ã§ã‚‚æ¬¡ã®éŸ³å£°ã‚’å†ç”Ÿ
        playNextInQueue();
      };
      await a.play();
    } catch (e) {
      console.error("Cartesia TTS error, fallback to Web Speech API:", e);
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ–ãƒ©ã‚¦ã‚¶ã® Web Speech API
      try {
        if (typeof window !== "undefined" && "speechSynthesis" in window) {
          const synth = window.speechSynthesis;

          const utt = new SpeechSynthesisUtterance(text);
          utt.lang = "ja-JP";
          utt.rate = 0.95;
          utt.pitch = 0.85;
          utt.volume = 1.0;

          const vs = voicesRef.current;
          if (vs && vs.length) {
            const preferMale = [
              "Otoya",
              "Hattori",
              "Google æ—¥æœ¬èª",
              "Microsoft Ichiro",
              "Kenji",
            ];
            const jaVoices = vs.filter(
              (v) =>
                /ja/i.test(v.lang || "") ||
                /æ—¥æœ¬èª|Japanese/i.test(v.name || "")
            );
            jaVoices.sort((a, b) => {
              const ai = preferMale.findIndex((p) =>
                (a.name || "").includes(p)
              );
              const bi = preferMale.findIndex((p) =>
                (b.name || "").includes(p)
              );
              return (ai === -1 ? 999 : ai) - (bi === -1 ? 999 : bi);
            });
            if (jaVoices[0]) {
              utt.voice = jaVoices[0];
            }
          }

          utt.onstart = () => {
            setIsSpeaking(true);
            isSpeakingRef.current = true;
            try {
              recognitionRef.current?.stop();
            } catch {}
            setIsRecording(false);
            startVolumeMonitoring(); // éŸ³é‡ç›£è¦–é–‹å§‹
          };
          utt.onend = () => {
            isPlayingQueueRef.current = false;
            onTtsEnded(); // çµ±ä¸€ã•ã‚ŒãŸçµ‚äº†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            playNextInQueue();
          };
          utt.onerror = () => {
            isPlayingQueueRef.current = false;
            onTtsEnded(); // çµ±ä¸€ã•ã‚ŒãŸçµ‚äº†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            playNextInQueue();
          };
          utteranceRef.current = utt;
          synth.speak(utt);
          return;
        }
      } catch {}
      setIsSpeaking(false);
      isPlayingQueueRef.current = false;
      playNextInQueue();
    }
  };

  const speak = (text: string) => {
    if (!text) return;
    // ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¦å†ç”Ÿé–‹å§‹
    audioQueueRef.current.push(text);
    playNextInQueue();
  };

  // TTSçµ‚äº†æ™‚ã®çµ±ä¸€ã•ã‚ŒãŸçµ‚äº†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const onTtsEnded = () => {
    setIsSpeaking(false);
    isSpeakingRef.current = false;
    stopVolumeMonitoring(); // éŸ³é‡ç›£è¦–åœæ­¢

    // å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†é–‹ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¤ãƒ™ãƒ³ãƒˆè¡çªã‚’é¿ã‘ã‚‹ï¼‰
    if (isContinuousListening && !isManualInputRef.current) {
      setTimeout(() => {
        try {
          recognitionRef.current?.start();
          setIsRecording(true);
        } catch {}
      }, 300);
    }
  };

  // è‡ªå‹•é€ä¿¡å‡¦ç†
  const handleAutoSubmit = async (text: string) => {
    if (!text.trim() || isLoading) return;

    // éŸ³å£°èªè­˜ã‚’ä¸€æ™‚åœæ­¢
    try {
      recognitionRef.current?.stop();
    } catch {}
    setIsRecording(false);

    // èª­ã¿ä¸Šã’ä¸­ãªã‚‰åœæ­¢
    cancelSpeaking();

    const userMessage: Message = {
      id: Date.now().toString(),
      role: "user",
      content: text,
    };

    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setIsLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: [...messages, userMessage].map((m) => ({
            role: m.role,
            content: m.content,
          })),
        }),
      });

      if (!response.ok) {
        const errText = await response.text().catch(() => "");
        console.error(`API Error (${response.status}):`, errText);
        throw new Error(
          `Failed to fetch (${response.status}): ${errText || "no body"}`
        );
      }

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      if (!reader) throw new Error("No reader");

      let assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: "assistant",
        content: "",
      };

      setMessages((prev) => [...prev, assistantMessage]);

      // ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç”¨ã®å¤‰æ•°ã¨ã‚¿ã‚¤ãƒãƒ¼
      let updateScheduled = false;
      const scheduleUpdate = () => {
        if (!updateScheduled) {
          updateScheduled = true;
          requestAnimationFrame(() => {
            setMessages((prev) => {
              const newMessages = [...prev];
              newMessages[newMessages.length - 1] = { ...assistantMessage };
              return newMessages;
            });
            updateScheduled = false;
          });
        }
      };

      // TTSå…ˆè¡Œé–‹å§‹ç”¨ã®å¤‰æ•°
      let lastSpokenIndex = 0;
      const sentenceEndPattern = /[ã€‚ï¼ï¼Ÿ\n]/;

      const checkAndSpeak = () => {
        if (!supportsTTS) return;

        const content = assistantMessage.content;
        for (let i = lastSpokenIndex; i < content.length; i++) {
          if (sentenceEndPattern.test(content[i])) {
            const textToSpeak = content.slice(lastSpokenIndex, i + 1).trim();
            if (textToSpeak) {
              speak(textToSpeak);
              lastSpokenIndex = i + 1;
            }
            break;
          }
        }
      };

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split("\n");

        for (const line of lines) {
          if (line.startsWith("0:")) {
            try {
              const jsonStr = line.substring(2);
              const data = JSON.parse(jsonStr);
              if (data && typeof data === "string") {
                assistantMessage.content += data;
                scheduleUpdate();
                checkAndSpeak();
              }
            } catch (e) {
              // JSON parse error, skip
            }
          }
        }
      }

      setMessages((prev) => {
        const newMessages = [...prev];
        newMessages[newMessages.length - 1] = { ...assistantMessage };
        return newMessages;
      });

      if (supportsTTS && lastSpokenIndex < assistantMessage.content.length) {
        const remainingText = assistantMessage.content
          .slice(lastSpokenIndex)
          .trim();
        if (remainingText) {
          speak(remainingText);
        }
      }
    } catch (error) {
      console.error("Error:", error);
      setMessages((prev) => [
        ...prev,
        {
          id: (Date.now() + 2).toString(),
          role: "assistant",
          content: "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        },
      ]);
    } finally {
      setIsLoading(false);

      // å¿œç­”å®Œäº†å¾Œã«éŸ³å£°èªè­˜ã‚’å†é–‹
      if (isContinuousListening && !isManualInputRef.current) {
        setTimeout(() => {
          try {
            recognitionRef.current?.start();
            setIsRecording(true);
          } catch (e) {
            console.log("Recognition restart after response failed:", e);
          }
        }, 1000);
      }
    }
  };

  // æ‰‹å‹•å…¥åŠ›æ™‚ã®åˆ¶å¾¡
  const handleInputFocus = () => {
    isManualInputRef.current = true;
    // éŸ³å£°èªè­˜ã‚’ä¸€æ™‚åœæ­¢
    try {
      recognitionRef.current?.stop();
    } catch {}
    setIsRecording(false);
  };

  const handleInputBlur = () => {
    isManualInputRef.current = false;
    // æ‰‹å‹•å…¥åŠ›ãŒç©ºãªã‚‰éŸ³å£°èªè­˜ã‚’å†é–‹
    if (!input.trim() && isContinuousListening) {
      setTimeout(() => {
        try {
          recognitionRef.current?.start();
          setIsRecording(true);
        } catch (e) {
          console.log("Recognition restart after manual input failed:", e);
        }
      }, 500);
    }
  };

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡å‡¦ç†
  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    // éŒ²éŸ³ä¸­ãªã‚‰é€ä¿¡å‰ã«åœæ­¢
    if (isRecording) {
      try {
        recognitionRef.current?.stop();
      } catch {}
      setIsRecording(false);
    }

    // èª­ã¿ä¸Šã’ä¸­ãªã‚‰åœæ­¢
    cancelSpeaking();

    // æš«å®šè¡¨ç¤ºã‚’å‰Šé™¤
    const finalInput = input.replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, "");

    const userMessage: Message = {
      id: Date.now().toString(),
      role: "user",
      content: finalInput,
    };

    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setIsLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: [...messages, userMessage].map((m) => ({
            role: m.role,
            content: m.content,
          })),
        }),
      });

      if (!response.ok) {
        const errText = await response.text().catch(() => "");
        console.error(`API Error (${response.status}):`, errText);
        throw new Error(
          `Failed to fetch (${response.status}): ${errText || "no body"}`
        );
      }

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      if (!reader) throw new Error("No reader");

      let assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: "assistant",
        content: "",
      };

      // è¡¨ç¤ºã¯ã—ãªã„ãŒã€ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿æŒ
      setMessages((prev) => [...prev, assistantMessage]);

      // ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç”¨ã®å¤‰æ•°ã¨ã‚¿ã‚¤ãƒãƒ¼
      let updateScheduled = false;
      const scheduleUpdate = () => {
        if (!updateScheduled) {
          updateScheduled = true;
          requestAnimationFrame(() => {
            setMessages((prev) => {
              const newMessages = [...prev];
              newMessages[newMessages.length - 1] = { ...assistantMessage };
              return newMessages;
            });
            updateScheduled = false;
          });
        }
      };

      // TTSå…ˆè¡Œé–‹å§‹ç”¨ã®å¤‰æ•°
      let lastSpokenIndex = 0; // æœ€å¾Œã«èª­ã¿ä¸Šã’ãŸä½ç½®
      const sentenceEndPattern = /[ã€‚ï¼ï¼Ÿ\n]/; // æ–‡æœ«åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³

      const checkAndSpeak = () => {
        if (!supportsTTS) return;

        const content = assistantMessage.content;
        // æœ€å¾Œã«èª­ã¿ä¸Šã’ãŸä½ç½®ä»¥é™ã§æ–‡æœ«ã‚’æ¢ã™
        for (let i = lastSpokenIndex; i < content.length; i++) {
          if (sentenceEndPattern.test(content[i])) {
            // æ–‡æœ«ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®éƒ¨åˆ†ã¾ã§ã‚’èª­ã¿ä¸Šã’
            const textToSpeak = content.slice(lastSpokenIndex, i + 1).trim();
            if (textToSpeak) {
              speak(textToSpeak);
              lastSpokenIndex = i + 1;
            }
            break; // 1æ–‡ãšã¤å‡¦ç†
          }
        }
      };

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split("\n");

        for (const line of lines) {
          if (line.startsWith("0:")) {
            try {
              const jsonStr = line.substring(2);
              const data = JSON.parse(jsonStr);
              if (data && typeof data === "string") {
                assistantMessage.content += data;
                // å³æ™‚æ›´æ–°ã§ã¯ãªãã€æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã¾ã¨ã‚ã¦æ›´æ–°
                scheduleUpdate();
                // æ–‡æœ«ãŒæ¥ãŸã‚‰å³åº§ã«TTSé–‹å§‹
                checkAndSpeak();
              }
            } catch (e) {
              // JSON parse error, skip
            }
          }
        }
      }

      // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†å¾Œã«æœ€çµ‚æ›´æ–°ã‚’ç¢ºå®Ÿã«åæ˜ 
      setMessages((prev) => {
        const newMessages = [...prev];
        newMessages[newMessages.length - 1] = { ...assistantMessage };
        return newMessages;
      });

      // æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°æœ€å¾Œã«èª­ã¿ä¸Šã’
      if (supportsTTS && lastSpokenIndex < assistantMessage.content.length) {
        const remainingText = assistantMessage.content
          .slice(lastSpokenIndex)
          .trim();
        if (remainingText) {
          speak(remainingText);
        }
      }
    } catch (error) {
      console.error("Error:", error);
      setMessages((prev) => [
        ...prev,
        {
          id: (Date.now() + 2).toString(),
          role: "assistant",
          content: "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        },
      ]);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="w-full h-screen flex flex-col bg-gradient-to-br from-gray-50 to-gray-100 dark:from-gray-900 dark:to-gray-800">
      {/* 3Dã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¡¨ç¤ºã‚¨ãƒªã‚¢ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰ */}
      <div className="flex-1 relative">
        <Canvas camera={{ position: [0, 0, 3.5], fov: 40 }}>
          <ambientLight intensity={0.5} />
          <directionalLight position={[5, 5, 5]} intensity={1} />
          <directionalLight position={[-5, 5, -5]} intensity={0.5} />

          <Suspense fallback={null}>
            <Dog position={[0, -1.7, 0]} scale={0.7} isTalking={isTalking} />
            <Environment preset="sunset" />
          </Suspense>
        </Canvas>
      </div>

      {/* ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›æ¬„ï¼ˆå°ã•ãã€ä¸­å¤®å¯„ã›ï¼‰ */}
      <div className="border-t border-gray-200 dark:border-gray-700 px-4 py-3 bg-white/80 dark:bg-gray-800/80 backdrop-blur">
        <form
          onSubmit={handleSubmit}
          className="max-w-md mx-auto flex items-center gap-2"
        >
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onFocus={handleInputFocus}
            onBlur={handleInputBlur}
            placeholder={
              isContinuousListening
                ? "è©±ã—ã‹ã‘ã‚‹ã‹ã€ã“ã“ã«æ–‡å­—ã‚’å…¥åŠ›..."
                : "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."
            }
            className="flex-1 px-3 py-2 border border-gray-300 dark:border-gray-600 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500 bg-white dark:bg-gray-700 text-gray-900 dark:text-white text-sm"
            disabled={isLoading}
          />
          {/* æ‰‹å‹•å…¥åŠ›æ™‚ã®ã¿é€ä¿¡ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º */}
          {input.trim() && (
            <button
              type="submit"
              disabled={isLoading}
              className="px-3 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:bg-gray-400 disabled:cursor-not-allowed transition-colors font-medium text-sm"
            >
              {isLoading ? "å¿œç­”ä¸­..." : "é€ä¿¡"}
            </button>
          )}
        </form>
        {/* å¸¸æ™‚ãƒªãƒƒã‚¹ãƒ³çŠ¶æ…‹ã®è¡¨ç¤º */}
        {isContinuousListening && !isManualInputRef.current && (
          <div className="text-center text-xs text-gray-500 dark:text-gray-400 mt-2">
            ğŸ¤ éŸ³å£°ã‚’èã„ã¦ã„ã¾ã™... è©±ã—ã‹ã‘ã¦ãã ã•ã„
          </div>
        )}
      </div>
    </div>
  );
}
