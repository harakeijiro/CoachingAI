"use client";

import { Suspense, useRef, useEffect, useState } from "react";
import { Canvas } from "@react-three/fiber";
import { Environment } from "@react-three/drei";
import { Dog } from "@/components/characters/mental/dog";
import AuthGuard from "@/components/auth/auth-guard";
import { requestMicrophonePermission, checkMicrophonePermissionState } from "@/lib/utils/microphone-permission";

type Message = {
  id: string;
  role: "user" | "assistant";
  content: string;
};

// Web Speech API ã®å‹å®šç¾©
interface ISpeechRecognition extends EventTarget {
  lang: string;
  interimResults: boolean;
  continuous: boolean;
  onresult: ((event: ISpeechRecognitionEvent) => void) | null;
  onend: (() => void) | null;
  onerror: ((event: ISpeechRecognitionErrorEvent) => void) | null;
  start: () => void;
  stop: () => void;
}

interface ISpeechRecognitionEvent extends Event {
  resultIndex: number;
  results: ISpeechRecognitionResultList;
}

interface ISpeechRecognitionResultList {
  length: number;
  item(index: number): ISpeechRecognitionResult;
  [index: number]: ISpeechRecognitionResult;
}

interface ISpeechRecognitionResult {
  isFinal: boolean;
  length: number;
  item(index: number): ISpeechRecognitionAlternative;
  [index: number]: ISpeechRecognitionAlternative;
}

interface ISpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

interface ISpeechRecognitionErrorEvent extends Event {
  error: string;
  message: string;
}

interface ISpeechRecognitionConstructor {
  new (): ISpeechRecognition;
}

function ChatPage() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [isLoading, setIsLoading] = useState(false);

  // ====== ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•é€ä¿¡ç”¨ è¿½åŠ  ======
  const formRef = useRef<HTMLFormElement | null>(null);
  const isComposingRef = useRef(false); // æ—¥æœ¬èªIMEå¤‰æ›ä¸­ã‚¬ãƒ¼ãƒ‰
  const autoSubmitTimerRef = useRef<NodeJS.Timeout | null>(null);
  const AUTO_DEBOUNCE_MS = 400; // â† ãƒ‡ãƒã‚¦ãƒ³ã‚¹
  const MIN_AUTO_CHARS = 4; // â† æœ€å°æ–‡å­—æ•°

  // ãƒãƒ£ãƒƒãƒˆæ¬„ã®æ‹¡å¼µçŠ¶æ…‹
  const [isExpanded, setIsExpanded] = useState(false);

  // ãƒã‚¤ã‚¯çŠ¶æ…‹ç›£è¦–ç”¨
  const [showMicPopup, setShowMicPopup] = useState(false);
  const [micPopupShown, setMicPopupShown] = useState(false); // ä¸€åº¦è¡¨ç¤ºã•ã‚ŒãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°

  // éŸ³å£°å…¥åŠ›é–¢é€£
  const recognitionRef = useRef<ISpeechRecognition | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [supportsSpeech, setSupportsSpeech] = useState(false);
  const [isContinuousListening, setIsContinuousListening] = useState(false);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const isManualInputRef = useRef<boolean>(false);

  // è¿½åŠ ï¼šç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆã®æ•´å½¢ãƒ»é‡è¤‡ã‚¬ãƒ¼ãƒ‰ãƒ»ï¼ˆéŸ³å£°çµŒè·¯ã§ã‚‚åˆ©ç”¨ï¼‰
  const normalize = (t: string) => t.replace(/\s+/g, " ").trim();
  const lastSentRef = useRef<string>("");
  const isSpeakingRef = useRef(false); // setIsSpeakingã«åŒæœŸ
  const shouldSend = (t: string) => {
    const text = normalize(t);
    if (text.length < 5) return false; // ãƒã‚¤ã‚ºé˜²æ­¢
    if (text === lastSentRef.current) return false; // åŒä¸€æŠ‘åˆ¶
    lastSentRef.current = text;
    return true;
  };

  // ====== éŸ³é‡ç›£è¦–ï¼ˆå‰²ã‚Šè¾¼ã¿ç”¨ï¼šã‚ãªãŸã®æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ ======
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const volumeCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const volumeThreshold = 0.01;

  const startVolumeMonitoring = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const AudioContextClass = window.AudioContext || (window as typeof window & { webkitAudioContext?: typeof AudioContext }).webkitAudioContext;
      if (!AudioContextClass) {
        console.error("AudioContext is not supported");
        return;
      }
      const audioContext = new AudioContextClass();
      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 256;
      microphone.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      microphoneRef.current = microphone;

      volumeCheckIntervalRef.current = setInterval(() => {
        if (isSpeaking && analyser) {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(dataArray);
          let sum = 0;
          for (let i = 0; i < dataArray.length; i++)
            sum += dataArray[i] * dataArray[i];
          const rms = Math.sqrt(sum / dataArray.length) / 255;

          if (rms > volumeThreshold) {
            console.log("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—å§‹ã‚ã¾ã—ãŸã€TTSã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«:", rms);
            cancelSpeaking();
            if (isContinuousListening && !isManualInputRef.current) {
              setTimeout(() => {
                try {
                  recognitionRef.current?.start();
                  setIsRecording(true);
                } catch (e) {
                  console.log(
                    "Recognition restart after interruption failed:",
                    e
                  );
                }
              }, 100);
            }
          }
        }
      }, 50);
    } catch (error) {
      console.error("Volume monitoring failed:", error);
      // ãƒã‚¤ã‚¯ã®è¨±å¯ãŒæ‹’å¦ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
      if (error instanceof DOMException && error.name === "NotAllowedError") {
        console.log("ãƒã‚¤ã‚¯ã®è¨±å¯ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚éŸ³é‡ç›£è¦–ã‚’åœæ­¢ã—ã¾ã™ã€‚");
        return;
      }
    }
  };

  const stopVolumeMonitoring = () => {
    if (volumeCheckIntervalRef.current) {
      clearInterval(volumeCheckIntervalRef.current);
      volumeCheckIntervalRef.current = null;
    }
    try {
      microphoneRef.current?.disconnect();
    } catch {}
    try {
      audioContextRef.current?.close();
    } catch {}
  };

  // éŸ³å£°åˆæˆï¼ˆTTSï¼‰é–¢é€£
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [supportsTTS, setSupportsTTS] = useState(false);
  const utteranceRef = useRef<SpeechSynthesisUtterance | null>(null);
  const voicesRef = useRef<SpeechSynthesisVoice[] | null>(null);
  const [audioEl, setAudioEl] = useState<HTMLAudioElement | null>(null);

  // éŸ³å£°ã‚­ãƒ¥ãƒ¼
  const audioQueueRef = useRef<string[]>([]);
  const isPlayingQueueRef = useRef(false);

  // 3Dã®å£ãƒ‘ã‚¯
  const isTalking = isSpeaking;

  // isSpeakingRefã‚’setIsSpeakingã¨åŒæœŸ
  useEffect(() => {
    isSpeakingRef.current = isSpeaking;
  }, [isSpeaking]);

  // ====== Web Speech API åˆæœŸåŒ–ï¼ˆã‚ãªãŸã®æ—¢å­˜ï¼‰ ======
  useEffect(() => {
    const SR =
      (typeof window !== "undefined" &&
        ((window as typeof window & { SpeechRecognition?: ISpeechRecognitionConstructor; webkitSpeechRecognition?: ISpeechRecognitionConstructor }).SpeechRecognition ||
          (window as typeof window & { SpeechRecognition?: ISpeechRecognitionConstructor; webkitSpeechRecognition?: ISpeechRecognitionConstructor }).webkitSpeechRecognition)) ||
      null;

    if (SR) {
      setSupportsSpeech(true);
      const recognition = new SR();
      recognition.lang = "ja-JP";
      recognition.interimResults = true;
      recognition.continuous = true;

      recognition.onresult = (event: ISpeechRecognitionEvent) => {
        let interim = "";
        let finalText = "";
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) finalText += transcript;
          else interim += transcript;
        }

        console.log("éŸ³å£°èªè­˜çµæœ:", {
          interim,
          finalText,
          isContinuousListening,
          isManualInput: isManualInputRef.current,
        });

        if (!isManualInputRef.current) {
          setInput((prev) => {
            const base = prev.replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, "");
            return finalText
              ? base + finalText
              : base + (interim ? `ï¼ˆè©±ã—ä¸­â€¦${interim}ï¼‰` : "");
          });

          // finalã¯éŸ³å£°çµŒè·¯ã®å³é€ä¿¡ï¼ˆæ—¢å­˜æ–¹é‡ï¼‰
          if (finalText && isContinuousListening) {
            const cleanText = finalText.trim();
            console.log(
              "finalTextæ¤œå‡º:",
              cleanText,
              "shouldSendçµæœ:",
              shouldSend(cleanText)
            );
            if (cleanText && shouldSend(cleanText)) {
              setInput("");
              setTimeout(() => {
                handleAutoSubmit(cleanText);
              }, 0);
            }
          }
        }
      };

      recognition.onend = () => {
        setIsRecording(false);
        setInput((prev) => prev.replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, ""));
        if (isContinuousListening && !isManualInputRef.current) {
          setTimeout(() => {
            try {
              recognitionRef.current?.start();
              setIsRecording(true);
            } catch (e) {
              console.log("Recognition restart failed:", e);
            }
          }, 100);
        }
      };

      recognition.onerror = (event: ISpeechRecognitionErrorEvent) => {
        console.log("Recognition error:", event.error);
        setIsRecording(false);

        // NotAllowedErrorã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å†è©¦è¡Œã‚’ä¿ƒã™
        if (event.error === "not-allowed") {
          console.log("ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
          setHasUserInteracted(false); // å†ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿ƒã™
          return;
        }

        if (
          isContinuousListening &&
          !isManualInputRef.current &&
          event.error !== "not-allowed"
        ) {
          setTimeout(() => {
            try {
              recognitionRef.current?.start();
              setIsRecording(true);
            } catch (e) {
              console.log("Recognition restart after error failed:", e);
            }
          }, 1000);
        }
      };

      recognitionRef.current = recognition;
    }

    return () => {
      try {
        recognitionRef.current?.stop();
      } catch {}
      const timeoutId = silenceTimeoutRef.current;
      if (timeoutId) clearTimeout(timeoutId);
      stopVolumeMonitoring();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ¤œçŸ¥ç”¨
  const [hasUserInteracted, setHasUserInteracted] = useState(false);

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ¤œçŸ¥
  useEffect(() => {
    const handleUserInteraction = async () => {
      setHasUserInteracted(true);
      
      // ãƒã‚¤ã‚¯ã®è¨±å¯ã‚’äº‹å‰ã«è¦æ±‚
      try {
        await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log("ãƒã‚¤ã‚¯ã®è¨±å¯ãŒäº‹å‰ã«å¾—ã‚‰ã‚Œã¾ã—ãŸ");
      } catch (e) {
        console.log("ãƒã‚¤ã‚¯ã®è¨±å¯ãŒäº‹å‰ã«æ‹’å¦ã•ã‚Œã¾ã—ãŸ:", e);
        // ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œï¼ˆå¾Œã§å†è©¦è¡Œã™ã‚‹ï¼‰
      }
      
      // ä¸€åº¦æ¤œçŸ¥ã—ãŸã‚‰ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’å‰Šé™¤
      document.removeEventListener("click", handleUserInteraction);
      document.removeEventListener("keydown", handleUserInteraction);
      document.removeEventListener("touchstart", handleUserInteraction);
    };

    // ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰æ™‚ã®è‡ªå‹•æŒ¨æ‹¶æ©Ÿèƒ½
    const autoGreeting = async () => {
      // å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…ã¤
      setTimeout(async () => {
        try {
          // ãƒã‚¤ã‚¯ã®è¨±å¯çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
          const micState = await checkMicrophonePermissionState();
          
          if (micState === "granted") {
            // æ—¢ã«è¨±å¯æ¸ˆã¿ã®å ´åˆã¯è‡ªå‹•çš„ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œçŸ¥
            console.log("ãƒã‚¤ã‚¯ãŒæ—¢ã«è¨±å¯æ¸ˆã¿ã§ã™ã€‚è‡ªå‹•çš„ã«éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚");
            setHasUserInteracted(true);
            
            // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚TTSã‚‚å®Ÿè¡Œã—ãªã„
            // const greetingMessage: Message = {
            //   id: Date.now().toString(),
            //   role: "assistant",
            //   content: "ã“ã‚“ã«ã¡ã¯ï¼è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚ä½•ã§ã‚‚ãŠèã‹ã›ãã ã•ã„ã€‚",
            // };
            // setMessages([greetingMessage]);
            
            // TTSã§æŒ¨æ‹¶ã‚’èª­ã¿ä¸Šã’ãªã„
            // if (supportsTTS) {
            //   setTimeout(() => {
            //     speak(greetingMessage.content);
            //   }, 1000);
            // }
          } else {
            // è¨±å¯ã•ã‚Œã¦ã„ãªã„å ´åˆã¯é€šå¸¸ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’è¨­å®š
            document.addEventListener("click", handleUserInteraction);
            document.addEventListener("keydown", handleUserInteraction);
            document.addEventListener("touchstart", handleUserInteraction);
            
            // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„
            // const greetingMessage: Message = {
            //   id: Date.now().toString(),
            //   role: "assistant",
            //   content: "ã“ã‚“ã«ã¡ã¯ï¼ç”»é¢ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‹ã‚‰è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚",
            // };
            // setMessages([greetingMessage]);
          }
        } catch (error) {
          console.log("ãƒã‚¤ã‚¯çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼:", error);
          // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é€šå¸¸ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’è¨­å®š
          document.addEventListener("click", handleUserInteraction);
          document.addEventListener("keydown", handleUserInteraction);
          document.addEventListener("touchstart", handleUserInteraction);
          
          // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„
          // const greetingMessage: Message = {
          //   id: Date.now().toString(),
          //   role: "assistant",
          //   content: "ã“ã‚“ã«ã¡ã¯ï¼ç”»é¢ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‹ã‚‰è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚",
          // };
          // setMessages([greetingMessage]);
        }
      }, 0); // å³åº§ã«è‡ªå‹•ãƒã‚§ãƒƒã‚¯
    };

    // è‡ªå‹•æŒ¨æ‹¶ã‚’é–‹å§‹
    autoGreeting();

    // ãƒã‚¤ã‚¯çŠ¶æ…‹ã‚’å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯
    const checkMicPermission = async () => {
      try {
        const state = await checkMicrophonePermissionState();
        
        // ãƒã‚¤ã‚¯ãŒæ‹’å¦ã•ã‚Œã¦ã„ã‚‹å ´åˆã§ã€ã¾ã ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’è¡¨ç¤ºã—ã¦ã„ãªã„å ´åˆã®ã¿è¡¨ç¤º
        if (state === "denied" && !micPopupShown) {
          setShowMicPopup(true);
          setMicPopupShown(true); // ä¸€åº¦è¡¨ç¤ºã—ãŸã“ã¨ã‚’è¨˜éŒ²
        }
      } catch (error) {
        console.error("ãƒã‚¤ã‚¯çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼:", error);
      }
    };

    // åˆå›ãƒã‚§ãƒƒã‚¯
    checkMicPermission();
    
    // å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ï¼ˆ30ç§’é–“éš”ï¼‰
    const micCheckInterval = setInterval(checkMicPermission, 30000);

    return () => {
      document.removeEventListener("click", handleUserInteraction);
      document.removeEventListener("keydown", handleUserInteraction);
      document.removeEventListener("touchstart", handleUserInteraction);
      clearInterval(micCheckInterval);
    };
  }, [supportsTTS]);

  // ãƒã‚¤ã‚¯è¨±å¯ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã®å‡¦ç†
  const handleMicPermissionRequest = async () => {
    try {
      const result = await requestMicrophonePermission();
      
      if (result.success && result.stream) {
        setShowMicPopup(false);
        // éŸ³å£°èªè­˜ã‚’é–‹å§‹
        recognitionRef.current?.start();
        setIsRecording(true);
        console.log("ãƒã‚¤ã‚¯è¨±å¯ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚");
      } else {
        alert(result.error || "ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
      }
    } catch (error) {
      console.error("ãƒã‚¤ã‚¯è¨±å¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—:", error);
      alert("ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
    }
  };

  const handleCloseMicPopup = () => {
    setShowMicPopup(false);
    setMicPopupShown(true); // é–‰ã˜ãŸæ™‚ã‚‚è¡¨ç¤ºæ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²
  };

  // å¸¸æ™‚ãƒªãƒƒã‚¹ãƒ³é–‹å§‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å¾Œï¼‰
  useEffect(() => {
    if (
      supportsSpeech &&
      recognitionRef.current &&
      !isContinuousListening &&
      hasUserInteracted
    ) {
      setIsContinuousListening(true);

      // ãƒã‚¤ã‚¯ã®è¨±å¯ã‚’æ±‚ã‚ã¦ã‹ã‚‰éŸ³å£°èªè­˜ã‚’é–‹å§‹
      const startRecognition = async () => {
        // æ–°ã—ã„ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ã‚¯è¨±å¯å‡¦ç†ã‚’ä½¿ç”¨
        const result = await requestMicrophonePermission();

        if (result.success && result.stream) {
          // éŸ³å£°èªè­˜ã‚’é–‹å§‹
          recognitionRef.current?.start();
          setIsRecording(true);
          console.log("éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã—ãŸ");
          
          // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - éŸ³å£°èªè­˜é–‹å§‹æ™‚ã®æŒ¨æ‹¶é€ä¿¡ã‚’åœæ­¢
          // if (messages.length === 0) {
          //   setTimeout(() => {
          //     const greetingMessage = "ã“ã‚“ã«ã¡ã¯ï¼è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚";
          //     handleAutoSubmit(greetingMessage);
          //   }, 2000); // 2ç§’å¾Œã«æŒ¨æ‹¶
          // }
        } else {
          // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
          alert(result.error || "ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
          setHasUserInteracted(false); // å†ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿ƒã™
        }
      };

      // æ—¢ã«ãƒã‚¤ã‚¯è¨±å¯ãŒå–å¾—æ¸ˆã¿ã®å ´åˆã¯å³åº§ã«é–‹å§‹ã€ãã†ã§ãªã‘ã‚Œã°å°‘ã—å¾…æ©Ÿ
      const delay = hasUserInteracted ? 100 : 500;
      setTimeout(startRecognition, delay);
    }
  }, [supportsSpeech, isContinuousListening, hasUserInteracted, messages.length]);

  // ====== TTS åˆæœŸåŒ–ï¼ˆã‚ãªãŸã®æ—¢å­˜ï¼‰ ======
  useEffect(() => {
    if (typeof window === "undefined") return;
    const hasTTS =
      "speechSynthesis" in window && "SpeechSynthesisUtterance" in window;
    setSupportsTTS(!!hasTTS);
    if (!hasTTS) return;

    const synth = window.speechSynthesis;
    const updateVoices = () => {
      const v = synth.getVoices();
      if (v && v.length) {
        voicesRef.current = v;
        const jaVoices = v.filter(
          (voice) =>
            /ja/i.test(voice.lang || "") ||
            /æ—¥æœ¬èª|Japanese/i.test(voice.name || "")
        );
        if (jaVoices.length > 0) {
          console.log("ğŸ“¢ åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒœã‚¤ã‚¹:");
          jaVoices.forEach((voice) =>
            console.log(`  - ${voice.name} (${voice.lang})`)
          );
        }
      }
    };
    updateVoices();
    synth.onvoiceschanged = updateVoices;

    return () => {
      try {
        synth.onvoiceschanged = null;
      } catch {}
      try {
        synth.cancel();
      } catch {}
    };
  }, []);

  // ====== TTS åˆ¶å¾¡ï¼ˆã‚ãªãŸã®æ—¢å­˜ï¼‹ãƒ•ãƒƒã‚¯ï¼‰ ======
  const cancelSpeaking = () => {
    audioQueueRef.current = [];
    isPlayingQueueRef.current = false;
    try {
      audioEl?.pause();
    } catch {}
    try {
      window?.speechSynthesis?.cancel?.();
    } catch {}
    setIsSpeaking(false);
  };

  const playNextInQueue = async () => {
    if (isPlayingQueueRef.current || audioQueueRef.current.length === 0) return;
    isPlayingQueueRef.current = true;
    const text = audioQueueRef.current.shift()!;

    try {
      const res = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text }),
      });
      if (!res.ok) throw new Error(`TTS ${res.status}`);
      const buf = await res.arrayBuffer();
      const blob = new Blob([buf], { type: "audio/wav" });
      const url = URL.createObjectURL(blob);
      const a = new Audio(url);
      setAudioEl(a);
      a.onplay = () => {
        setIsSpeaking(true);
        isSpeakingRef.current = true;
        try {
          recognitionRef.current?.stop();
        } catch {}
        setIsRecording(false);
        startVolumeMonitoring();
      };
      a.onended = () => {
        URL.revokeObjectURL(url);
        isPlayingQueueRef.current = false;
        onTtsEnded();
        playNextInQueue();
      };
      a.onerror = () => {
        URL.revokeObjectURL(url);
        isPlayingQueueRef.current = false;
        onTtsEnded();
        playNextInQueue();
      };
      await a.play();
    } catch (e) {
      console.error("Cartesia TTS error, fallback to Web Speech API:", e);
      try {
        if (typeof window !== "undefined" && "speechSynthesis" in window) {
          const synth = window.speechSynthesis;
          const utt = new SpeechSynthesisUtterance(text);
          utt.lang = "ja-JP";
          utt.rate = 0.95;
          utt.pitch = 0.85;
          utt.volume = 1.0;
          const vs = voicesRef.current;
          if (vs && vs.length) {
            const preferMale = [
              "Otoya",
              "Hattori",
              "Google æ—¥æœ¬èª",
              "Microsoft Ichiro",
              "Kenji",
            ];
            const jaVoices = vs.filter(
              (v) =>
                /ja/i.test(v.lang || "") ||
                /æ—¥æœ¬èª|Japanese/i.test(v.name || "")
            );
            jaVoices.sort((a, b) => {
              const ai = preferMale.findIndex((p) =>
                (a.name || "").includes(p)
              );
              const bi = preferMale.findIndex((p) =>
                (b.name || "").includes(p)
              );
              return (ai === -1 ? 999 : ai) - (bi === -1 ? 999 : bi);
            });
            if (jaVoices[0]) utt.voice = jaVoices[0];
          }
          utt.onstart = () => {
            setIsSpeaking(true);
            isSpeakingRef.current = true;
            try {
              recognitionRef.current?.stop();
            } catch {}
            setIsRecording(false);
            startVolumeMonitoring();
          };
          utt.onend = () => {
            isPlayingQueueRef.current = false;
            onTtsEnded();
            playNextInQueue();
          };
          utt.onerror = () => {
            isPlayingQueueRef.current = false;
            onTtsEnded();
            playNextInQueue();
          };
          utteranceRef.current = utt;
          synth.speak(utt);
          return;
        }
      } catch {}
      setIsSpeaking(false);
      isPlayingQueueRef.current = false;
      playNextInQueue();
    }
  };

  const speak = (text: string) => {
    if (!text) return;
    audioQueueRef.current.push(text);
    playNextInQueue();
  };

  const onTtsEnded = () => {
    setIsSpeaking(false);
    isSpeakingRef.current = false;
    stopVolumeMonitoring();
    if (isContinuousListening && !isManualInputRef.current) {
      setTimeout(() => {
        try {
          recognitionRef.current?.start();
          setIsRecording(true);
        } catch {}
      }, 300);
    }
  };

  // ====== è‡ªå‹•é€ä¿¡å‡¦ç†ï¼ˆæ—¢å­˜ï¼‰ ======
  const handleAutoSubmit = async (text: string) => {
    if (!text.trim() || isLoading) return;
    try {
      recognitionRef.current?.stop();
    } catch {}
    setIsRecording(false);
    cancelSpeaking();

    const userMessage: Message = {
      id: Date.now().toString(),
      role: "user",
      content: text,
    };
    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setIsLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: [...messages, userMessage].map((m) => ({
            role: m.role,
            content: m.content,
          })),
        }),
      });

      if (!response.ok) {
        const errText = await response.text().catch(() => "");
        console.error(`API Error (${response.status}):`, errText);
        throw new Error(
          `Failed to fetch (${response.status}): ${errText || "no body"}`
        );
      }

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      if (!reader) throw new Error("No reader");

      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: "assistant",
        content: "",
      };
      setMessages((prev) => [...prev, assistantMessage]);

      let updateScheduled = false;
      const scheduleUpdate = () => {
        if (!updateScheduled) {
          updateScheduled = true;
          requestAnimationFrame(() => {
            setMessages((prev) => {
              const newMessages = [...prev];
              newMessages[newMessages.length - 1] = { ...assistantMessage };
              return newMessages;
            });
            updateScheduled = false;
          });
        }
      };

      let lastSpokenIndex = 0;
      const sentenceEndPattern = /[ã€‚ï¼ï¼Ÿ\n]/;
      const checkAndSpeak = () => {
        if (!supportsTTS) return;
        const content = assistantMessage.content;
        for (let i = lastSpokenIndex; i < content.length; i++) {
          if (sentenceEndPattern.test(content[i])) {
            const textToSpeak = content.slice(lastSpokenIndex, i + 1).trim();
            if (textToSpeak) {
              speak(textToSpeak);
              lastSpokenIndex = i + 1;
            }
            break;
          }
        }
      };

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        const chunk = decoder.decode(value);
        const lines = chunk.split("\n");
        for (const line of lines) {
          if (line.startsWith("0:")) {
            try {
              const jsonStr = line.substring(2);
              const data = JSON.parse(jsonStr);
              if (data && typeof data === "string") {
                assistantMessage.content += data;
                scheduleUpdate();
                checkAndSpeak();
              }
            } catch {}
          }
        }
      }

      setMessages((prev) => {
        const newMessages = [...prev];
        newMessages[newMessages.length - 1] = { ...assistantMessage };
        return newMessages;
      });
      if (supportsTTS && lastSpokenIndex < assistantMessage.content.length) {
        const remainingText = assistantMessage.content
          .slice(lastSpokenIndex)
          .trim();
        if (remainingText) speak(remainingText);
      }
    } catch (error) {
      console.error("Error:", error);
      setMessages((prev) => [
        ...prev,
        {
          id: (Date.now() + 2).toString(),
          role: "assistant",
          content: "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        },
      ]);
    } finally {
      setIsLoading(false);
      if (isContinuousListening && !isManualInputRef.current) {
        setTimeout(() => {
          try {
            recognitionRef.current?.start();
            setIsRecording(true);
          } catch (e) {
            console.log("Recognition restart after response failed:", e);
          }
        }, 1000);
      }
    }
  };

  // ====== æ‰‹å‹•å…¥åŠ›æ™‚ã®åˆ¶å¾¡ï¼ˆæ—¢å­˜ï¼‹IMEãƒ•ãƒ©ã‚°ï¼‰ ======
  const handleInputFocus = () => {
    isManualInputRef.current = true;
    try {
      recognitionRef.current?.stop();
    } catch {}
    setIsRecording(false);
  };
  const handleInputBlur = () => {
    isManualInputRef.current = false;
    if (!input.trim() && isContinuousListening) {
      setTimeout(() => {
        try {
          recognitionRef.current?.start();
          setIsRecording(true);
        } catch (e) {
          console.log("Recognition restart after manual input failed:", e);
        }
      }, 500);
    }
  };

  // ====== â˜…ã“ã“ãŒã€Œæ–‡å­—ãŒå…¥ã£ãŸã‚‰è‡ªå‹•ã§é€ä¿¡ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€å®Ÿè£… ======
  useEffect(() => {
    // å…¥åŠ›æ›´æ–°ã®ãŸã³ã«ãƒ‡ãƒã‚¦ãƒ³ã‚¹
    if (autoSubmitTimerRef.current) clearTimeout(autoSubmitTimerRef.current);

    // ç©ºãªã‚‰ä½•ã‚‚ã—ãªã„
    if (!input) return;

    autoSubmitTimerRef.current = setTimeout(() => {
      // ã‚¬ãƒ¼ãƒ‰æ¡ä»¶ï¼šé€ä¿¡ã—ã¦ã‚ˆã„çŠ¶æ…‹ã‹
      if (isLoading) return; // é€ä¿¡ä¸­ã¯ä¸å¯
      if (isComposingRef.current) return; // æ—¥æœ¬èªå¤‰æ›ä¸­ã¯ä¸å¯
      if (/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u.test(input)) return; // æš«å®šè¡¨ç¤ºã¯ä¸å¯
      const clean = normalize(input);
      if (clean.length < MIN_AUTO_CHARS) return; // çŸ­ã™ãã‚‹
      if (!shouldSend(clean)) return; // ç›´å‰ã¨åŒä¸€ãªã©

      // å®Ÿéš›ã«ã€Œé€ä¿¡ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€ã®ã¨åŒã˜å‹•ä½œ
      formRef.current?.requestSubmit();
    }, AUTO_DEBOUNCE_MS);

    return () => {
      if (autoSubmitTimerRef.current) clearTimeout(autoSubmitTimerRef.current);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [input, isLoading]);

  // ====== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆæ‰‹å‹•ãƒœã‚¿ãƒ³/è‡ªå‹•requestSubmit å…±é€šï¼‰ ======
  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    if (isRecording) {
      try {
        recognitionRef.current?.stop();
      } catch {}
      setIsRecording(false);
    }
    cancelSpeaking();

    const finalInput = input.replace(/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u, "");
    const userMessage: Message = {
      id: Date.now().toString(),
      role: "user",
      content: finalInput,
    };

    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setIsLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: [...messages, userMessage].map((m) => ({
            role: m.role,
            content: m.content,
          })),
        }),
      });

      if (!response.ok) {
        const errText = await response.text().catch(() => "");
        console.error(`API Error (${response.status}):`, errText);
        throw new Error(
          `Failed to fetch (${response.status}): ${errText || "no body"}`
        );
      }

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      if (!reader) throw new Error("No reader");

      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: "assistant",
        content: "",
      };
      setMessages((prev) => [...prev, assistantMessage]);

      let updateScheduled = false;
      const scheduleUpdate = () => {
        if (!updateScheduled) {
          updateScheduled = true;
          requestAnimationFrame(() => {
            setMessages((prev) => {
              const newMessages = [...prev];
              newMessages[newMessages.length - 1] = { ...assistantMessage };
              return newMessages;
            });
            updateScheduled = false;
          });
        }
      };

      let lastSpokenIndex = 0;
      const sentenceEndPattern = /[ã€‚ï¼ï¼Ÿ\n]/;
      const checkAndSpeak = () => {
        if (!supportsTTS) return;
        const content = assistantMessage.content;
        for (let i = lastSpokenIndex; i < content.length; i++) {
          if (sentenceEndPattern.test(content[i])) {
            const textToSpeak = content.slice(lastSpokenIndex, i + 1).trim();
            if (textToSpeak) {
              speak(textToSpeak);
              lastSpokenIndex = i + 1;
            }
            break;
          }
        }
      };

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        const chunk = decoder.decode(value);
        const lines = chunk.split("\n");
        for (const line of lines) {
          if (line.startsWith("0:")) {
            try {
              const jsonStr = line.substring(2);
              const data = JSON.parse(jsonStr);
              if (data && typeof data === "string") {
                assistantMessage.content += data;
                scheduleUpdate();
                checkAndSpeak();
              }
            } catch {}
          }
        }
      }

      setMessages((prev) => {
        const newMessages = [...prev];
        newMessages[newMessages.length - 1] = { ...assistantMessage };
        return newMessages;
      });
      if (supportsTTS && lastSpokenIndex < assistantMessage.content.length) {
        const remainingText = assistantMessage.content
          .slice(lastSpokenIndex)
          .trim();
        if (remainingText) speak(remainingText);
      }
    } catch (error) {
      console.error("Error:", error);
      setMessages((prev) => [
        ...prev,
        {
          id: (Date.now() + 2).toString(),
          role: "assistant",
          content: "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        },
      ]);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="w-full h-screen flex flex-col">
      {/* 3Dã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¡¨ç¤ºã‚¨ãƒªã‚¢ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰ */}
      <div className="h-screen relative">
        <Canvas camera={{ position: [0, 0, 3.5], fov: 40 }}>
          <ambientLight intensity={0.5} />
          <directionalLight position={[5, 5, 5]} intensity={1} />
          <directionalLight position={[-5, 5, -5]} intensity={0.5} />
          <Suspense fallback={null}>
            <Dog position={[0, -1.7, 0]} scale={0.7} isTalking={isTalking} />
            <Environment preset="sunset" />
          </Suspense>
        </Canvas>
      </div>

      {/* ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›æ¬„ï¼ˆå°ã•ãã€ä¸­å¤®å¯„ã›ï¼‰ */}
        <div className="absolute bottom-0 left-0 right-0 px-3 py-2">
          <div className={`mx-auto relative transition-all duration-300 ${isExpanded ? 'max-w-lg' : 'max-w-40'}`}>
            <form
              ref={formRef} // â† â˜… è¿½åŠ ï¼šè‡ªå‹•submitç”¨
              onSubmit={handleSubmit}
              className="relative"
            >
              <input
                type="text"
                value={input}
                onChange={(e) => setInput(e.target.value)}
                onFocus={() => {
                  handleInputFocus();
                  setIsExpanded(true);
                }}
                onBlur={() => {
                  handleInputBlur();
                  if (!input.trim()) {
                    setIsExpanded(false);
                  }
                }}
                onCompositionStart={() => {
                  isComposingRef.current = true;
                }} // â† â˜… IMEé–‹å§‹
                onCompositionEnd={(e) => {
                  isComposingRef.current = false;
                  setInput(e.currentTarget.value);
                }} // â† â˜… IMEç¢ºå®š
                placeholder={
                  isExpanded
                    ? ""
                    : isContinuousListening
                    ? "è©±ã—ã‹ã‘ã¦ã¿ã¦"
                    : "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."
                }
                className={`w-full px-4 py-3 border border-gray-300/30 dark:border-gray-600/30 rounded-full focus:outline-none focus:ring-2 focus:ring-blue-500 bg-white/20 dark:bg-gray-800/20 backdrop-blur-md text-gray-900 dark:text-white text-sm transition-all duration-300 ${isExpanded ? 'pr-12 text-left' : 'text-center'}`}
                disabled={isLoading}
              />
              {isExpanded && (
                <button
                  type="submit"
                  disabled={isLoading}
                  className="absolute right-2 top-1/2 transform -translate-y-1/2 w-10 h-10 bg-white/20 dark:bg-gray-800/20 backdrop-blur-md text-gray-900 dark:text-white rounded-full hover:bg-white/30 dark:hover:bg-gray-800/30 disabled:bg-gray-400/80 disabled:cursor-not-allowed transition-colors font-semibold text-lg flex items-center justify-center"
                >
                  {isLoading ? "å¿œç­”ä¸­..." : "â†‘"}
                </button>
              )}
            </form>
            {!isExpanded && (
              <button
                type="submit"
                disabled={isLoading}
                onClick={handleSubmit}
                className="absolute right-0 top-1/2 transform -translate-y-1/2 w-10 h-10 bg-white text-black rounded-full hover:bg-gray-100 disabled:bg-gray-400 disabled:cursor-not-allowed transition-colors font-semibold text-lg flex items-center justify-center opacity-0 pointer-events-none"
              >
                {isLoading ? "å¿œç­”ä¸­..." : "â†‘"}
              </button>
            )}
          </div>

        {hasUserInteracted && !isContinuousListening && (
          <div className="text-center text-xs text-gray-500 dark:text-gray-400 mt-1">
            ğŸ¤ éŸ³å£°èªè­˜ã‚’é–‹å§‹ä¸­...
          </div>
        )}
      </div>

      {/* ãƒã‚¤ã‚¯è¨±å¯ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ— */}
      {showMicPopup && (
        <div className="fixed inset-0 bg-black/50 flex items-center justify-center p-4 z-50">
          <div className="bg-white dark:bg-gray-800 rounded-xl shadow-xl p-6 w-full max-w-sm text-center">
            <div className="text-5xl mb-3">ğŸ¤</div>

            <h2 className="text-lg font-bold text-gray-900 dark:text-white mb-2">
              ãƒã‚¤ã‚¯ã‚’ã‚ªãƒ³ã«ã—ã¦ãã ã•ã„
            </h2>

            <p className="text-sm text-gray-600 dark:text-gray-400 mb-4 leading-relaxed">
              éŸ³å£°ã§ä¼šè©±ã™ã‚‹ã«ã¯ãƒã‚¤ã‚¯ã®ä½¿ç”¨è¨±å¯ãŒå¿…è¦ã§ã™ã€‚
              <br />
              ã€Œãƒã‚¤ã‚¯ã‚’è¨±å¯ã™ã‚‹ã€ã‚’æŠ¼ã™ã¨ã€ãƒ–ãƒ©ã‚¦ã‚¶ãŒè¨±å¯ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
            </p>

            <div className="flex flex-col gap-3">
              <button
                onClick={handleMicPermissionRequest}
                className="w-full bg-blue-600 text-white font-semibold py-2 rounded-lg hover:bg-blue-700"
              >
                ãƒã‚¤ã‚¯ã‚’è¨±å¯ã™ã‚‹
              </button>

              <button
                onClick={handleCloseMicPopup}
                className="w-full text-gray-500 dark:text-gray-400 text-xs underline"
              >
                å¾Œã§è¨­å®šã™ã‚‹
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}

export default function ChatPageWithAuth() {
  return (
    <AuthGuard>
      <ChatPage />
    </AuthGuard>
  );
}
