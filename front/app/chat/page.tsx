/**
 * ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã®ä¼šè©±ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸
 * - WhisperéŸ³å£°èªè­˜ã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±
 * - 3Dã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¡¨ç¤ºã¨å£ãƒ‘ã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
 * - ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨éŸ³å£°å…¥åŠ›ã®ä¸¡å¯¾å¿œ
 * - TTSã«ã‚ˆã‚‹éŸ³å£°å¿œç­”
 */
"use client";

import { Suspense, useRef, useEffect, useState, useCallback } from "react";
import { Canvas } from "@react-three/fiber";
import { Environment } from "@react-three/drei";
import { useRouter } from "next/navigation";
import { Dog } from "@/components/characters/mental/dog";
import AuthGuard from "@/components/auth/auth-guard";
import { requestMicrophonePermission, checkMicrophonePermissionState } from "@/lib/utils/microphone-permission";
import { useWhisper } from "@/lib/hooks/useWhisper";
import { useTTS } from "@/lib/hooks/useTTS";
import { useChat, type Message } from "@/lib/hooks/useChat";
import { ChatInput } from "@/components/chat/ChatInput";
import { MicrophonePermissionPopup } from "@/components/chat/MicrophonePermissionPopup";



// STEP2-1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‹ã®å®šç¾©ï¼ˆpending ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ä»˜ãï¼‰
type VoiceMessage = {
  id: string;
  role: "user";
  text: string;
  pending: boolean;
};

function ChatPage() {
  const router = useRouter();

  // 1. stateã‚„refé¡
  const [input, setInput] = useState("");
  const [voiceInput, setVoiceInput] = useState(""); // éŸ³å£°èªè­˜å°‚ç”¨ã®çŠ¶æ…‹
  const [isVoiceEnabled, setIsVoiceEnabled] = useState(true); // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ONï¼ˆè‡ªå‹•éŒ²éŸ³é–‹å§‹ï¼‰
  const isVoiceEnabledRef = useRef(isVoiceEnabled); // refã§ã‚‚ç®¡ç†ï¼ˆuseWhisperã«æ¸¡ã™ãŸã‚ï¼‰
  const [isContinuousListening, setIsContinuousListening] = useState(false); // æ‰‹å‹•éŒ²éŸ³ãƒ¢ãƒ¼ãƒ‰
  
  // isVoiceEnabledãŒå¤‰æ›´ã•ã‚ŒãŸã‚‰refã‚‚æ›´æ–°
  useEffect(() => {
    isVoiceEnabledRef.current = isVoiceEnabled;
  }, [isVoiceEnabled]);
  
  // STEP2-2: voiceMessages state ã‚’è¿½åŠ 
  const [voiceMessages, setVoiceMessages] = useState<VoiceMessage[]>([]);
  
  const isManualInputRef = useRef<boolean>(false);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const speechBufferRef = useRef<string>("");
  const lastSentVoiceTextRef = useRef<string>(""); // æœ€å¾Œã«é€ä¿¡ã—ãŸéŸ³å£°èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ

  // ====== ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•é€ä¿¡ç”¨ è¿½åŠ  ======
  const formRef = useRef<HTMLFormElement | null>(null);
  const isComposingRef = useRef(false); // æ—¥æœ¬èªIMEå¤‰æ›ä¸­ã‚¬ãƒ¼ãƒ‰
  const autoSubmitTimerRef = useRef<NodeJS.Timeout | null>(null);
  const AUTO_DEBOUNCE_MS = 400; // â† ãƒ‡ãƒã‚¦ãƒ³ã‚¹
  const MIN_AUTO_CHARS = 4; // â† æœ€å°æ–‡å­—æ•°

  // ãƒã‚¤ã‚¯çŠ¶æ…‹ç›£è¦–ç”¨
  const [showMicPopup, setShowMicPopup] = useState(false);
  const [micPopupShown, setMicPopupShown] = useState(false); // ä¸€åº¦è¡¨ç¤ºã•ã‚ŒãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°

  // è¿½åŠ ï¼šç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆã®æ•´å½¢ãƒ»é‡è¤‡ã‚¬ãƒ¼ãƒ‰ãƒ»ï¼ˆéŸ³å£°çµŒè·¯ã§ã‚‚åˆ©ç”¨ï¼‰
  const normalize = (t: string) => t.replace(/\s+/g, " ").trim();
  const lastSentRef = useRef<string>("");
  const shouldSend = useCallback((t: string) => {
    const text = normalize(t);
    if (text.length < 5) return false; // ãƒã‚¤ã‚ºé˜²æ­¢
    if (text === lastSentRef.current) return false; // åŒä¸€æŠ‘åˆ¶
    lastSentRef.current = text;
    return true;
  }, []);

  // ====== å…±é€šé–¢æ•° ======
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const volumeCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const volumeThreshold = 0.01;

  const startVolumeMonitoring = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const AudioContextClass = window.AudioContext || (window as typeof window & { webkitAudioContext?: typeof AudioContext }).webkitAudioContext;
      if (!AudioContextClass) {
        console.error("AudioContext is not supported");
        return;
      }
      const audioContext = new AudioContextClass();
      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 256;
      microphone.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      microphoneRef.current = microphone;

      volumeCheckIntervalRef.current = setInterval(() => {
        if (isSpeaking && analyser) {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(dataArray);
          let sum = 0;
          for (let i = 0; i < dataArray.length; i++)
            sum += dataArray[i] * dataArray[i];
          const rms = Math.sqrt(sum / dataArray.length) / 255;

          if (rms > volumeThreshold) {
            cancelSpeaking();
            if (isContinuousListening && !isManualInputRef.current) {
              setTimeout(() => {
                startRecognition();
              }, 100);
            }
          }
        }
      }, 50);
    } catch (error) {
      console.error("Volume monitoring failed:", error);
      // ãƒã‚¤ã‚¯ã®è¨±å¯ãŒæ‹’å¦ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
      if (error instanceof DOMException && error.name === "NotAllowedError") {
        return;
      }
    }
  };

  const stopVolumeMonitoring = () => {
    if (volumeCheckIntervalRef.current) {
      clearInterval(volumeCheckIntervalRef.current);
      volumeCheckIntervalRef.current = null;
    }
    try {
      microphoneRef.current?.disconnect();
    } catch {}
    try {
      audioContextRef.current?.close();
    } catch {}
  };

  // 2. isSpeakingRefã‚’å¤–éƒ¨ã§ä½œæˆï¼ˆã“ã‚ŒãŒå”¯ä¸€ã®çœŸå®Ÿï¼‰
  const isSpeakingRef = useRef<boolean>(false);
  
  // STEP2-3: onResult é–¢æ•°ã®å®Ÿè£…
  // é‡è¤‡å‘¼ã³å‡ºã—ã‚’é˜²ããŸã‚ã®refï¼ˆmessageIdå˜ä½ã§ç®¡ç†ï¼‰
  const processingMessageIdsRef = useRef<Set<string>>(new Set());
  
  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è‡ªå‹•å‰Šé™¤ç”¨ã‚¿ã‚¤ãƒãƒ¼ï¼ˆmessageIdå˜ä½ã§ç®¡ç†ï¼‰
  const messageClearTimersRef = useRef<Map<string, NodeJS.Timeout>>(new Map());
  
  const handleWhisperResult = useCallback((messageId: string, text: string) => {
    // ç©ºæ–‡å­—ã®å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
    if (text === "") {
      // æ—¢å­˜ã®ã‚¿ã‚¤ãƒãƒ¼ãŒã‚ã‚Œã°ã‚¯ãƒªã‚¢
      const existingTimer = messageClearTimersRef.current.get(messageId);
      if (existingTimer) {
        clearTimeout(existingTimer);
        messageClearTimersRef.current.delete(messageId);
      }
      
      setVoiceMessages((prev) => {
        return prev.filter((msg) => msg.id !== messageId);
      });
      processingMessageIdsRef.current.delete(messageId);
      return;
    }
    
    // ä»®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ"â€¦"ï¼‰ã®é‡è¤‡å‘¼ã³å‡ºã—ã‚’é˜²ãï¼ˆrefã§å³åº§ã«ãƒã‚§ãƒƒã‚¯ï¼‰
    const isPendingText = text === "â€¦";
    if (isPendingText && processingMessageIdsRef.current.has(messageId)) {
      return;
    }
    
    // æ—¢å­˜ã®ã‚¿ã‚¤ãƒãƒ¼ãŒã‚ã‚Œã°ã‚¯ãƒªã‚¢ï¼ˆsetStateã®å‰ã«å®Ÿè¡Œï¼‰
    const existingTimer = messageClearTimersRef.current.get(messageId);
    if (existingTimer) {
      clearTimeout(existingTimer);
      messageClearTimersRef.current.delete(messageId);
    }
    
    setVoiceMessages((prev) => {
      // 1. ãã® id ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã¾ã ãªã‘ã‚Œã°ã€æ–°è¦ push { id: messageId, role:"user", text, pending:true }
      const existingIndex = prev.findIndex((msg) => msg.id === messageId);
      
      if (existingIndex === -1) {
        // æ–°è¦ä½œæˆï¼ˆä»®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ "â€¦" ã®å ´åˆï¼‰
        // ä»®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆã¯å‡¦ç†ä¸­ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹ï¼ˆsetStateã®å‰ã§å®Ÿè¡Œï¼‰
        if (isPendingText) {
          processingMessageIdsRef.current.add(messageId);
        }
        
        const newMessage = {
          id: messageId,
          role: "user" as const,
          text: text,
          pending: isPendingText,
        };
        return [...prev, newMessage];
      } else {
        // 2. ãã® id ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã™ã§ã«ã‚ã‚Œã°ã€ãã®è¦ç´ ã® text ã‚’ text ã§ä¸Šæ›¸ãã—ã€pending: false ã«æ›´æ–°
        const newMessages = [...prev];
        
        // å®Ÿãƒ†ã‚­ã‚¹ãƒˆãŒæ¥ãŸã‚‰å‡¦ç†ä¸­ãƒ•ãƒ©ã‚°ã‚’è§£é™¤ï¼ˆsetStateã®å‰ã§å®Ÿè¡Œï¼‰
        if (!isPendingText) {
          processingMessageIdsRef.current.delete(messageId);
        }
        
        newMessages[existingIndex] = {
          ...newMessages[existingIndex],
          text: text,
          pending: isPendingText,
        };
        return newMessages;
      }
    });
    
    // setStateã®å¾Œã«ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®šï¼ˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤–ã§å®Ÿè¡Œï¼‰
    // ä»®ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯å®Ÿãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã€2ç§’å¾Œã«å‰Šé™¤ã™ã‚‹ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š
    const timer = setTimeout(() => {
      setVoiceMessages((prev) => {
        return prev.filter((msg) => msg.id !== messageId);
      });
      messageClearTimersRef.current.delete(messageId);
      processingMessageIdsRef.current.delete(messageId);
    }, 2000); // 2ç§’å¾Œ
    
    messageClearTimersRef.current.set(messageId, timer);
  }, []);
  
  // 3. useWhisper ã‚’ã“ã“ã§å‘¼ã¶ï¼ˆæ–°ã—ã„å½¢å¼ï¼‰
  const {
    isRecording,
    isSpeaking,
    startRecording,
    stopRecording,
    cancelRecording,
    sessionId,
  } = useWhisper({
    onResult: (messageId, text) => {
      // æ–°ã—ã„å½¢å¼ã§ onResult ã‚’å‘¼ã¶
      handleWhisperResult(messageId, text);
    },
    onError: (error) => {
      console.error("[useWhisper onError]", error);
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤º
      // 503ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹
      alert(error);
    },
    onTtsEnd: async () => {
      // TTSçµ‚äº†å¾Œã¯è‡ªå‹•éŒ²éŸ³ã‚’é–‹å§‹ã—ãªã„
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«è©±ã—ã‹ã‘ã‚‹ã¾ã§å¾…ã¤
    },
    isVoiceEnabled: () => isVoiceEnabledRef.current, // ãƒã‚¤ã‚¯ã®ã‚ªãƒ³/ã‚ªãƒ•çŠ¶æ…‹ã‚’æ¸¡ã™
  });

  // Whisperãƒ™ãƒ¼ã‚¹ãªã®ã§ã€restartRecognitionã€startRecognitionã¯ä¸è¦ï¼ˆãƒ€ãƒŸãƒ¼ã‚’æä¾›ï¼‰
  const supportsSpeech = true; // Whisperã¯å¸¸ã«åˆ©ç”¨å¯èƒ½
  const stopRecognition = stopRecording; // stopRecordingã‚’stopRecognitionã¨ã—ã¦ä½¿ç”¨
  const startRecognition = startRecording; // startRecordingã‚’startRecognitionã¨ã—ã¦ä½¿ç”¨
  const restartRecognition = (delay?: number, context?: string) => {
    // Whisper mode: è‡ªå‹•å†é–‹ã¯ä¸è¦ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å‹•ã§éŒ²éŸ³ã‚’é–‹å§‹ï¼‰
  };

  // 4. useTTS ã‚’å‘¼ã¶ï¼ˆisSpeakingRefã‚’æ¸¡ã™ï¼‰
  const {
    isSpeaking: ttsIsSpeaking,
    supportsTTS,
    speak,
    cancelSpeaking,
  } = useTTS({
    isSpeakingRef, // åŒã˜refã‚’æ¸¡ã™
    onTtsStart: () => {
      // TTSé–‹å§‹ â†’ éŸ³å£°èªè­˜åœæ­¢ï¼ˆbeginSpeakingå†…ã§å‡¦ç†ï¼‰
    },
    onTtsEnd: () => {
      // TTSçµ‚äº† â†’ éŸ³å£°èªè­˜å†é–‹ï¼ˆendSpeakingå†…ã§å‡¦ç†ï¼‰
    },
    stopRecognition,
    startVolumeMonitoring,
    stopVolumeMonitoring,
    restartRecognition,
  });

  // 3Dã®å£ãƒ‘ã‚¯ï¼ˆWhisperã®isSpeakingã‚’ä½¿ç”¨ï¼‰
  const isTalking = ttsIsSpeaking || isSpeaking;

  const {
    messages,
    setMessages,
    isLoading,
    sendMessage,
    handleAutoSubmit,
  } = useChat({
    stopRecognition,       // â† æœ¬ç‰©
    cancelSpeaking,
    speak,
    supportsTTS,
    restartRecognition,    // â† æœ¬ç‰©
  });

  // æ³¨: å¤ã„ handleSpeechResult ã¨ onResultRef ã¯å‰Šé™¤ã—ã¾ã—ãŸ
  // æ–°ã—ã„å®Ÿè£…ã§ã¯ useWhisper ã® onResult ãŒç›´æ¥ handleWhisperResult ã‚’å‘¼ã³ã¾ã™
  // voiceMessages state ã‚’ UI ã§è¡¨ç¤ºã—ã¦ãã ã•ã„


  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
  useEffect(() => {
    return () => {
      // silenceTimeoutã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
      const silenceTimeout = silenceTimeoutRef.current;
      if (silenceTimeout) clearTimeout(silenceTimeout);
      
      stopVolumeMonitoring();
      
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒãƒ¼ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæœ€æ–°ã®çŠ¶æ…‹ã‚’å–å¾—ï¼‰
      // eslint-disable-next-line react-hooks/exhaustive-deps
      messageClearTimersRef.current.forEach((timer) => {
        clearTimeout(timer);
      });
      // eslint-disable-next-line react-hooks/exhaustive-deps
      messageClearTimersRef.current.clear();
    };
  }, []);

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ¤œçŸ¥ç”¨
  const [hasUserInteracted, setHasUserInteracted] = useState(true); // æœ€åˆã‹ã‚‰ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ¸ˆã¿ã¨ã—ã¦æ‰±ã†

  // ãƒã‚¤ã‚¯ã‚’è‡ªå‹•é–‹å§‹
  useEffect(() => {
    const autoStartMic = async () => {
      try {
        const result = await requestMicrophonePermission();
        if (result.success && result.stream) {
          await startRecording();
        }
      } catch (error) {
        console.error("[è‡ªå‹•ãƒã‚¤ã‚¯é–‹å§‹] ã‚¨ãƒ©ãƒ¼:", error);
      }
    };

    const timer = setTimeout(() => {
      autoStartMic();
    }, 1000);

    return () => clearTimeout(timer);
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []); // åˆå›ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚ã®ã¿å®Ÿè¡Œ

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ¤œçŸ¥
  useEffect(() => {
    const handleUserInteraction = async () => {
      setHasUserInteracted(true);
      
      // ãƒã‚¤ã‚¯ã®è¨±å¯ã‚’äº‹å‰ã«è¦æ±‚
      try {
        await navigator.mediaDevices.getUserMedia({ audio: true });
      } catch (e) {
        // ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œï¼ˆå¾Œã§å†è©¦è¡Œã™ã‚‹ï¼‰
      }
      
      // ä¸€åº¦æ¤œçŸ¥ã—ãŸã‚‰ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’å‰Šé™¤
      document.removeEventListener("click", handleUserInteraction);
      document.removeEventListener("keydown", handleUserInteraction);
      document.removeEventListener("touchstart", handleUserInteraction);
    };

    // ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰æ™‚ã®è‡ªå‹•æŒ¨æ‹¶æ©Ÿèƒ½
    const autoGreeting = async () => {
      // å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…ã¤
      setTimeout(async () => {
        try {
          // ãƒã‚¤ã‚¯ã®è¨±å¯çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
          const micState = await checkMicrophonePermissionState();
          
          if (micState === "granted") {
            // æ—¢ã«è¨±å¯æ¸ˆã¿ã®å ´åˆã¯è‡ªå‹•çš„ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œçŸ¥
            setHasUserInteracted(true);
            
            // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚TTSã‚‚å®Ÿè¡Œã—ãªã„
            // const greetingMessage: Message = {
            //   id: Date.now().toString(),
            //   role: "assistant",
            //   content: "ã“ã‚“ã«ã¡ã¯ï¼è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚ä½•ã§ã‚‚ãŠèã‹ã›ãã ã•ã„ã€‚",
            // };
            // setMessages([greetingMessage]);
            
            // TTSã§æŒ¨æ‹¶ã‚’èª­ã¿ä¸Šã’ãªã„
            // if (supportsTTS) {
            //   setTimeout(() => {
            //     speak(greetingMessage.content);
            //   }, 1000);
            // }
          } else {
            // è¨±å¯ã•ã‚Œã¦ã„ãªã„å ´åˆã¯é€šå¸¸ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’è¨­å®š
            document.addEventListener("click", handleUserInteraction);
            document.addEventListener("keydown", handleUserInteraction);
            document.addEventListener("touchstart", handleUserInteraction);
            
            // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„
            // const greetingMessage: Message = {
            //   id: Date.now().toString(),
            //   role: "assistant",
            //   content: "ã“ã‚“ã«ã¡ã¯ï¼ç”»é¢ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‹ã‚‰è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚",
            // };
            // setMessages([greetingMessage]);
          }
        } catch (error) {
          // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é€šå¸¸ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’è¨­å®š
          document.addEventListener("click", handleUserInteraction);
          document.addEventListener("keydown", handleUserInteraction);
          document.addEventListener("touchstart", handleUserInteraction);
          
          // è‡ªå‹•æŒ¨æ‹¶ã‚’ç„¡åŠ¹åŒ– - ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„
          // const greetingMessage: Message = {
          //   id: Date.now().toString(),
          //   role: "assistant",
          //   content: "ã“ã‚“ã«ã¡ã¯ï¼ç”»é¢ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‹ã‚‰è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„ã€‚",
          // };
          // setMessages([greetingMessage]);
        }
      }, 0); // å³åº§ã«è‡ªå‹•ãƒã‚§ãƒƒã‚¯
    };

    // è‡ªå‹•æŒ¨æ‹¶ã‚’é–‹å§‹
    autoGreeting();

    // ãƒã‚¤ã‚¯çŠ¶æ…‹ã‚’å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯
    const checkMicPermission = async () => {
      try {
        const state = await checkMicrophonePermissionState();
        
        // ãƒã‚¤ã‚¯ãŒæ‹’å¦ã•ã‚Œã¦ã„ã‚‹å ´åˆã§ã€ã¾ã ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’è¡¨ç¤ºã—ã¦ã„ãªã„å ´åˆã®ã¿è¡¨ç¤º
        if (state === "denied" && !micPopupShown) {
          setShowMicPopup(true);
          setMicPopupShown(true); // ä¸€åº¦è¡¨ç¤ºã—ãŸã“ã¨ã‚’è¨˜éŒ²
        }
      } catch (error) {
        console.error("ãƒã‚¤ã‚¯çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼:", error);
      }
    };

    // åˆå›ãƒã‚§ãƒƒã‚¯
    checkMicPermission();
    
    // å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ï¼ˆ30ç§’é–“éš”ï¼‰
    const micCheckInterval = setInterval(checkMicPermission, 30000);

    return () => {
      document.removeEventListener("click", handleUserInteraction);
      document.removeEventListener("keydown", handleUserInteraction);
      document.removeEventListener("touchstart", handleUserInteraction);
      clearInterval(micCheckInterval);
    };
  }, [supportsTTS]);

  // ãƒã‚¤ã‚¯è¨±å¯ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã®å‡¦ç†
  const handleMicPermissionRequest = async () => {
    try {
      const result = await requestMicrophonePermission();
      
      if (result.success && result.stream) {
        setShowMicPopup(false);
        startRecognition();
      } else {
        alert(result.error || "ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
      }
    } catch (error) {
      console.error("ãƒã‚¤ã‚¯è¨±å¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—:", error);
      alert("ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
    }
  };

  const handleCloseMicPopup = () => {
    setShowMicPopup(false);
    setMicPopupShown(true); // é–‰ã˜ãŸæ™‚ã‚‚è¡¨ç¤ºæ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²
  };

  // å¸¸æ™‚ãƒªãƒƒã‚¹ãƒ³é–‹å§‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å¾Œï¼‰
  useEffect(() => {
    if (
      supportsSpeech &&
      !isContinuousListening &&
      hasUserInteracted
    ) {
      const startRecognitionWithPermission = async () => {
        const result = await requestMicrophonePermission();

        if (result.success && result.stream) {
          startRecognition();
        } else {
          alert(result.error || "ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
          setHasUserInteracted(false);
        }
      };

      const delay = hasUserInteracted ? 100 : 500;
      setTimeout(startRecognitionWithPermission, delay);
    }
  }, [supportsSpeech, isContinuousListening, hasUserInteracted, messages.length]);

  // ====== æ‰‹å‹•å…¥åŠ›æ™‚ã®åˆ¶å¾¡ï¼ˆæ—¢å­˜ï¼‹IMEãƒ•ãƒ©ã‚°ï¼‰ ======
  const handleInputFocus = () => {
    isManualInputRef.current = true;
    stopRecognition();
  };
  const handleInputBlur = () => {
    // å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦ã‹ã‚‰æ‰‹å‹•å…¥åŠ›ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
    setTimeout(() => {
      isManualInputRef.current = false;
      if (isContinuousListening && isVoiceEnabled) {
        restartRecognition(500, "after manual input blur");
      }
    }, 100);
  };

  // ====== â˜…ã“ã“ãŒã€Œæ–‡å­—ãŒå…¥ã£ãŸã‚‰è‡ªå‹•ã§é€ä¿¡ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€å®Ÿè£… ======
  useEffect(() => {
    // å…¥åŠ›æ›´æ–°ã®ãŸã³ã«ãƒ‡ãƒã‚¦ãƒ³ã‚¹
    if (autoSubmitTimerRef.current) clearTimeout(autoSubmitTimerRef.current);

    // ç©ºãªã‚‰ä½•ã‚‚ã—ãªã„
    if (!input) return;

    autoSubmitTimerRef.current = setTimeout(() => {
      // ã‚¬ãƒ¼ãƒ‰æ¡ä»¶ï¼šé€ä¿¡ã—ã¦ã‚ˆã„çŠ¶æ…‹ã‹
      if (isLoading) return; // é€ä¿¡ä¸­ã¯ä¸å¯
      if (isComposingRef.current) return; // æ—¥æœ¬èªå¤‰æ›ä¸­ã¯ä¸å¯
      if (/ï¼ˆè©±ã—ä¸­â€¦.*ï¼‰$/u.test(input)) return; // æš«å®šè¡¨ç¤ºã¯ä¸å¯
      const clean = normalize(input);
      if (clean.length < MIN_AUTO_CHARS) return; // çŸ­ã™ãã‚‹
      if (!shouldSend(clean)) return; // ç›´å‰ã¨åŒä¸€ãªã©

      // å®Ÿéš›ã«ã€Œé€ä¿¡ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€ã®ã¨åŒã˜å‹•ä½œ
      formRef.current?.requestSubmit();
    }, AUTO_DEBOUNCE_MS);

    return () => {
      if (autoSubmitTimerRef.current) clearTimeout(autoSubmitTimerRef.current);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [input, isLoading]);

  // ====== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆæ‰‹å‹•ãƒœã‚¿ãƒ³/è‡ªå‹•requestSubmit å…±é€šï¼‰ ======
  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    await sendMessage(input, false);
    setInput("");
    
    // ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡å¾Œã«éŸ³å£°èªè­˜ã‚’å†é–‹
    setTimeout(() => {
      isManualInputRef.current = false;
      if (isContinuousListening && isVoiceEnabled) {
        restartRecognition(500, "after text submit");
      }
    }, 200);
  };

  // ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠç”»é¢ã«æˆ»ã‚‹é–¢æ•°
  const handleBackToCharacterSelect = () => {
    // ç¾åœ¨ã®ãƒ†ãƒ¼ãƒã‚’å–å¾—ï¼ˆlocalStorageã‹ã‚‰ï¼‰
    const theme = localStorage.getItem("coaching_ai_default_theme") || "mental";
    router.push(`/character-select?theme=${theme}`);
  };

  // ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯å‡¦ç†ï¼ˆéŸ³å£°èªè­˜ã®æœ‰åŠ¹/ç„¡åŠ¹åˆ‡ã‚Šæ›¿ãˆï¼‰
    const handleMicButtonClick = async () => {
    
    if (isVoiceEnabled) {
      // ğŸ”´ ç¾åœ¨ON â†’ OFFã«ã™ã‚‹ï¼ˆéŒ²éŸ³åœæ­¢â†’é€ä¿¡ï¼‰
      stopRecording(); // éŒ²éŸ³åœæ­¢â†’Whisperé€ä¿¡
      setIsVoiceEnabled(false);
    } else {
      // ğŸŸ¢ ç¾åœ¨OFF â†’ ONã«ã™ã‚‹ï¼ˆéŒ²éŸ³é–‹å§‹ï¼‰
      // 1. ãƒã‚¤ã‚¯è¨±å¯ãƒã‚§ãƒƒã‚¯/ç¢ºä¿
      const result = await requestMicrophonePermission();
      if (!result.success || !result.stream) {
        alert(result.error || "ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™");
        return;
      }

      // 2. éŒ²éŸ³é–‹å§‹ã‚’ãƒˆãƒ©ã‚¤ï¼ˆawaitã—ã¦å®Œäº†ã‚’å¾…ã¤ï¼‰
      try {
        await startRecording();
        setIsVoiceEnabled(true);
      } catch (error) {
        console.error("éŒ²éŸ³é–‹å§‹ã«å¤±æ•—:", error);
        alert("éŒ²éŸ³ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ");
      }
    }
  };

  return (
    <div className="w-full h-screen flex flex-col">
      {/* æˆ»ã‚‹ãƒœã‚¿ãƒ³ */}
      <div className="absolute top-4 left-4 z-10">
        <button
          onClick={handleBackToCharacterSelect}
          className="bg-white/20 dark:bg-gray-800/20 backdrop-blur-md text-gray-900 dark:text-white rounded-full p-3 hover:bg-white/30 dark:hover:bg-gray-800/30 transition-all duration-200 shadow-lg"
          title="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠã«æˆ»ã‚‹"
        >
          <svg 
            className="w-6 h-6" 
            fill="none" 
            stroke="currentColor" 
            viewBox="0 0 24 24"
          >
            <path 
              strokeLinecap="round" 
              strokeLinejoin="round" 
              strokeWidth={2} 
              d="M15 19l-7-7 7-7" 
            />
          </svg>
        </button>
      </div>

      {/* 3Dã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¡¨ç¤ºã‚¨ãƒªã‚¢ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰ */}
      <div className="h-screen relative">
        <Canvas camera={{ position: [0, 0, 3.5], fov: 40 }}>
          <ambientLight intensity={0.5} />
          <directionalLight position={[5, 5, 5]} intensity={1} />
          <directionalLight position={[-5, 5, -5]} intensity={0.5} />
          <Suspense fallback={null}>
            <Dog position={[0, -1.7, 0]} scale={0.7} isTalking={isTalking} />
            <Environment preset="sunset" />
          </Suspense>
        </Canvas>
      </div>

      {/* ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ãƒ­ã‚°ï¼ˆè‡ªåˆ†å´å¹ãå‡ºã—ï¼‰- Canvasã®å¤–å´ã«é…ç½® */}
      <div className="fixed bottom-16 right-0 left-0 max-h-[40vh] overflow-y-auto px-4 flex flex-col items-center gap-2 pointer-events-none z-50">
        {voiceMessages.map((msg) => (
          <div
            key={msg.id}
            className={`w-full flex items-center justify-center text-base text-white transition-all duration-300 ${
              msg.pending
                ? "opacity-90"
                : "opacity-100"
            }`}
            style={{
              textShadow: '1px 1px 2px rgba(0,0,0,0.8)',
              minHeight: '40px',
            }}
          >
            <span className="font-medium">{msg.text}</span>
          </div>
        ))}
      </div>

      {/* ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ï¼ˆãƒãƒ£ãƒƒãƒˆæ¬„ã®å°‘ã—å³ï¼‰ */}
      <ChatInput
        input={input}
        setInput={setInput}
        isLoading={isLoading}
        isVoiceEnabled={isVoiceEnabled}
        isContinuousListening={isContinuousListening}
        supportsSpeech={supportsSpeech}
        onSubmit={handleSubmit}
        onInputFocus={handleInputFocus}
        onInputBlur={handleInputBlur}
        onMicButtonClick={handleMicButtonClick}
        voiceInput={voiceInput}
        setVoiceInput={setVoiceInput}
      />

      {/* ãƒã‚¤ã‚¯è¨±å¯ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ— */}
      <MicrophonePermissionPopup
        showMicPopup={showMicPopup}
        onMicPermissionRequest={handleMicPermissionRequest}
        onCloseMicPopup={handleCloseMicPopup}
      />
    </div>
  );
}

export default function ChatPageWithAuth() {
  return (
    <AuthGuard>
      <ChatPage />
    </AuthGuard>
  );
}
