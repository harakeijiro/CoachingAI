/**
 * éŸ³å£°åˆæˆï¼ˆTTSï¼‰ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
 * - Cartesia TTS APIã«ã‚ˆã‚‹é«˜å“è³ªéŸ³å£°ç”Ÿæˆ
 * - Web Speech APIã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
 * - éŸ³å£°ã‚­ãƒ¥ãƒ¼ç®¡ç†ã«ã‚ˆã‚‹é€£ç¶šå†ç”Ÿ
 * - éŸ³å£°å†ç”Ÿä¸­ã®çŠ¶æ…‹ç®¡ç†ã¨éŸ³å£°èªè­˜åˆ¶å¾¡
 */
"use client";

import { useRef, useState, useEffect } from "react";

interface UseTTSProps {
  isSpeakingRef?: React.MutableRefObject<boolean>; // å¤–éƒ¨ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹refï¼ˆå…±æœ‰ç”¨ï¼‰
  onTtsStart?: () => void;
  onTtsEnd?: () => void;
  stopRecognition?: () => void;
  startVolumeMonitoring?: () => void;
  stopVolumeMonitoring?: () => void;
  restartRecognition?: (delay: number, context: string) => void;
}

export const useTTS = ({
  isSpeakingRef: externalIsSpeakingRef,
  onTtsStart,
  onTtsEnd,
  stopRecognition,
  startVolumeMonitoring,
  stopVolumeMonitoring,
  restartRecognition,
}: UseTTSProps = {}) => {
  // TTSé–¢é€£ã®çŠ¶æ…‹
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [supportsTTS, setSupportsTTS] = useState(false);
  const utteranceRef = useRef<SpeechSynthesisUtterance | null>(null);
  const voicesRef = useRef<SpeechSynthesisVoice[] | null>(null);
  const [audioEl, setAudioEl] = useState<HTMLAudioElement | null>(null);

  // éŸ³å£°ã‚­ãƒ¥ãƒ¼
  const audioQueueRef = useRef<string[]>([]);
  const isPlayingQueueRef = useRef(false);

  // isSpeakingRefã‚’setIsSpeakingã¨åŒæœŸï¼ˆå¤–éƒ¨ã®refãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ã†ï¼‰
  const internalIsSpeakingRef = useRef(false);
  const isSpeakingRef = externalIsSpeakingRef || internalIsSpeakingRef;
  
  useEffect(() => {
    if (!externalIsSpeakingRef) {
      isSpeakingRef.current = isSpeaking;
    }
  }, [isSpeaking, externalIsSpeakingRef, isSpeakingRef]);

  // TTS åˆæœŸåŒ–
  useEffect(() => {
    if (typeof window === "undefined") return;
    const hasTTS =
      "speechSynthesis" in window && "SpeechSynthesisUtterance" in window;
    setSupportsTTS(!!hasTTS);
    if (!hasTTS) return;

    const synth = window.speechSynthesis;
    const updateVoices = () => {
      const v = synth.getVoices();
      if (v && v.length) {
        voicesRef.current = v;
        const jaVoices = v.filter(
          (voice) =>
            /ja/i.test(voice.lang || "") ||
            /æ—¥æœ¬èª|Japanese/i.test(voice.name || "")
        );
        if (jaVoices.length > 0) {
          console.log("ğŸ“¢ åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒœã‚¤ã‚¹:");
          jaVoices.forEach((voice) =>
            console.log(`  - ${voice.name} (${voice.lang})`)
          );
        }
      }
    };
    updateVoices();
    synth.onvoiceschanged = updateVoices;

    return () => {
      try {
        synth.onvoiceschanged = null;
      } catch {}
      try {
        synth.cancel();
      } catch {}
    };
  }, []);

  // TTS åˆ¶å¾¡ - é–‹å§‹å‡¦ç†ï¼ˆisSpeakingRefã‚’å³åº§ã«æ›´æ–°ï¼‰
  const beginSpeaking = () => {
    console.log("[useTTS] beginSpeaking: éŸ³å£°å†ç”Ÿé–‹å§‹");
    setIsSpeaking(true);
    isSpeakingRef.current = true;  // â† å³åº§ã«æ›´æ–°
    
    // éŸ³å£°èªè­˜ã‚’ç¢ºå®Ÿã«åœæ­¢
    if (stopRecognition) {
      try {
        stopRecognition();
        console.log("[useTTS] éŸ³å£°èªè­˜åœæ­¢å®Œäº†");
      } catch (error) {
        console.error("[useTTS] stopRecognition error:", error);
      }
    }
    
    startVolumeMonitoring?.();
    onTtsStart?.();
  };

  // TTS åˆ¶å¾¡ - çµ‚äº†å‡¦ç†ï¼ˆisSpeakingRefã‚’å³åº§ã«æ›´æ–°ï¼‰
  const endSpeaking = () => {
    console.log("[useTTS] endSpeaking: éŸ³å£°å†ç”Ÿå®Œäº†");
    setIsSpeaking(false);
    isSpeakingRef.current = false;  // â† å³åº§ã«æ›´æ–°
    stopVolumeMonitoring?.();
    
    // éŸ³å£°èªè­˜ã‚’å†é–‹
    if (restartRecognition) {
      try {
        restartRecognition(500, "after TTS ended");
        console.log("[useTTS] éŸ³å£°èªè­˜å†é–‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Œäº†");
      } catch (error) {
        console.error("[useTTS] restartRecognition error:", error);
      }
    }
    
    onTtsEnd?.();
  };

  const cancelSpeaking = () => {
    audioQueueRef.current = [];
    isPlayingQueueRef.current = false;
    try {
      audioEl?.pause();
    } catch {}
    try {
      window?.speechSynthesis?.cancel?.();
    } catch {}
    setIsSpeaking(false);
    isSpeakingRef.current = false;  // â† å³åº§ã«æ›´æ–°
  };

  const playNextInQueue = async () => {
    if (isPlayingQueueRef.current || audioQueueRef.current.length === 0) return;
    isPlayingQueueRef.current = true;
    const text = audioQueueRef.current.shift()!;

    try {
      const res = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text }),
      });
      if (!res.ok) throw new Error(`TTS ${res.status}`);
      const buf = await res.arrayBuffer();
      const blob = new Blob([buf], { type: "audio/wav" });
      const url = URL.createObjectURL(blob);
      const a = new Audio(url);
      setAudioEl(a);
      a.onplay = () => {
        beginSpeaking();
      };
      a.onended = () => {
        URL.revokeObjectURL(url);
        isPlayingQueueRef.current = false;
        endSpeaking();
        playNextInQueue();
      };
      a.onerror = () => {
        URL.revokeObjectURL(url);
        isPlayingQueueRef.current = false;
        endSpeaking();
        playNextInQueue();
      };
      await a.play();
    } catch (e) {
      console.error("Cartesia TTS error, fallback to Web Speech API:", e);
      try {
        if (typeof window !== "undefined" && "speechSynthesis" in window) {
          const synth = window.speechSynthesis;
          const utt = new SpeechSynthesisUtterance(text);
          utt.lang = "ja-JP";
          utt.rate = 0.95;
          utt.pitch = 0.85;
          utt.volume = 1.0;
          const vs = voicesRef.current;
          if (vs && vs.length) {
            const preferMale = [
              "Otoya",
              "Hattori",
              "Google æ—¥æœ¬èª",
              "Microsoft Ichiro",
              "Kenji",
            ];
            const jaVoices = vs.filter(
              (v) =>
                /ja/i.test(v.lang || "") ||
                /æ—¥æœ¬èª|Japanese/i.test(v.name || "")
            );
            jaVoices.sort((a, b) => {
              const ai = preferMale.findIndex((p) =>
                (a.name || "").includes(p)
              );
              const bi = preferMale.findIndex((p) =>
                (b.name || "").includes(p)
              );
              return (ai === -1 ? 999 : ai) - (bi === -1 ? 999 : bi);
            });
            if (jaVoices[0]) utt.voice = jaVoices[0];
          }
          utt.onstart = () => {
            beginSpeaking();
          };
          utt.onend = () => {
            isPlayingQueueRef.current = false;
            endSpeaking();
            playNextInQueue();
          };
          utt.onerror = () => {
            isPlayingQueueRef.current = false;
            endSpeaking();
            playNextInQueue();
          };
          utteranceRef.current = utt;
          synth.speak(utt);
          return;
        }
      } catch {}
      setIsSpeaking(false);
      isPlayingQueueRef.current = false;
      playNextInQueue();
    }
  };

  const speak = (text: string) => {
    if (!text) return;
    audioQueueRef.current.push(text);
    playNextInQueue();
  };

  return {
    isSpeaking,
    supportsTTS,
    speak,
    cancelSpeaking,
    isSpeakingRef,
  };
};
